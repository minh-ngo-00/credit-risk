{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir='parquet_files/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_applprev_1_0.parquet',\n",
       " 'train_applprev_1_1.parquet',\n",
       " 'train_applprev_2.parquet',\n",
       " 'train_base.parquet',\n",
       " 'train_credit_bureau_a_1_0.parquet',\n",
       " 'train_credit_bureau_a_1_1.parquet',\n",
       " 'train_credit_bureau_a_1_2.parquet',\n",
       " 'train_credit_bureau_a_1_3.parquet',\n",
       " 'train_credit_bureau_a_2_0.parquet',\n",
       " 'train_credit_bureau_a_2_1.parquet',\n",
       " 'train_credit_bureau_a_2_10.parquet',\n",
       " 'train_credit_bureau_a_2_2.parquet',\n",
       " 'train_credit_bureau_a_2_3.parquet',\n",
       " 'train_credit_bureau_a_2_4.parquet',\n",
       " 'train_credit_bureau_a_2_5.parquet',\n",
       " 'train_credit_bureau_a_2_6.parquet',\n",
       " 'train_credit_bureau_a_2_7.parquet',\n",
       " 'train_credit_bureau_a_2_8.parquet',\n",
       " 'train_credit_bureau_a_2_9.parquet',\n",
       " 'train_credit_bureau_b_1.parquet',\n",
       " 'train_credit_bureau_b_2.parquet',\n",
       " 'train_debitcard_1.parquet',\n",
       " 'train_deposit_1.parquet',\n",
       " 'train_other_1.parquet',\n",
       " 'train_person_1.parquet',\n",
       " 'train_person_2.parquet',\n",
       " 'train_static_0_0.parquet',\n",
       " 'train_static_0_1.parquet',\n",
       " 'train_static_cb_0.parquet',\n",
       " 'train_tax_registry_a_1.parquet',\n",
       " 'train_tax_registry_b_1.parquet',\n",
       " 'train_tax_registry_c_1.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=['train_base.parquet',\n",
    "       'train_person_1.parquet',\n",
    "       'train_person_2.parquet',                                                                                 \n",
    "    'train_applprev_1*.parquet',\n",
    "             'train_applprev_2.parquet',\n",
    "             'train_credit_bureau_a_1*.parquet',\n",
    "             'train_credit_bureau_a_2*.parquet',\n",
    "             'train_credit_bureau_b_1.parquet',\n",
    "             'train_credit_bureau_b_2.parquet',\n",
    "             'train_debitcard_1.parquet',\n",
    "             'train_deposit_1.parquet',\n",
    "             'train_other_1.parquet',\n",
    "             'train_static_0*.parquet',\n",
    "             'train_static_cb_0.parquet',\n",
    "             'train_tax_registry_a_1.parquet',\n",
    "             'train_tax_registry_b_1.parquet',\n",
    "             'train_tax_registry_c_1.parquet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_data(df):\n",
    "    for col in df.columns:\n",
    "        if (col[-1]=='D') | (col=='date_decision'):\n",
    "            df=df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        elif df[col].dtype==pl.Boolean:\n",
    "            df=df.with_columns(pl.col(col).cast(pl.String))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df):\n",
    "    base_cols=[col for col in df.columns if col in ['case_id', 'date_decision', 'WEEK_NUM', 'MONTH', 'target']]\n",
    "    base_agg=[pl.col(col).max() for col in base_cols if col !='case_id']\n",
    "    \n",
    "    cat_cols=[col for col in df.columns if (df[col].dtype ==pl.String) | (df[col].dtype==pl.Null) ]\n",
    "    cat_max=[pl.col(col).max().alias('max_'+col) for col in cat_cols]\n",
    "    cat_first=[pl.col(col).first().alias('first_'+col) for col in cat_cols]\n",
    "    cat_last=[pl.col(col).last().alias('last_'+col) for col in cat_cols]\n",
    "    cat_n_unique=[pl.col(col).n_unique().alias('n_unique_'+col) for col in cat_cols]\n",
    "    \n",
    "    d_cols=[col for col in df.columns if (df[col].dtype ==pl.Date)&(col!='date_decision')]\n",
    "    d_max=[pl.col(col).max().alias('max_'+col) for col in d_cols]\n",
    "    d_min=[pl.col(col).min().alias('min_'+col) for col in d_cols]\n",
    "    d_median=[pl.col(col).median().alias('median_'+col) for col in d_cols]\n",
    "\n",
    "    other_cols=[col for col in df.columns if col not in base_cols+cat_cols+d_cols]\n",
    "    max_other=[pl.col(col).max().alias('max_'+col) for col in other_cols]\n",
    "    mean_other=[pl.col(col).mean().alias('mean_'+col) for col in other_cols]\n",
    "    min_other=[pl.col(col).min().alias('min_'+col) for col in other_cols]\n",
    "    total=base_agg+cat_max+cat_first+cat_last+cat_n_unique+d_max+d_min+d_median+max_other+mean_other+min_other\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files, dir_):\n",
    "    PATH=Path(dir_)\n",
    "    for file in files:\n",
    "        paths=PATH.glob(file)\n",
    "        chunks=[]\n",
    "        for path in paths:\n",
    "            print(path)\n",
    "            df=pl.read_parquet(path)\n",
    "            df=cast_data(df)\n",
    "            if ('1' in file)|('2' in file): \n",
    "                df=df.group_by('case_id').agg(aggregate_data(df))\n",
    "            chunks.append(df)        \n",
    "        df=pl.concat(chunks, how='vertical_relaxed')\n",
    "        if file in ['train_base.parquet','test_base.parquet']:\n",
    "            base=df\n",
    "            name=file\n",
    "        else:\n",
    "            for col in df.columns:\n",
    "                if 'num_group' in col:\n",
    "                    if name=='train_base.parquet':\n",
    "                        df=df.rename({col:col+file[5:]})\n",
    "                    if name=='test_base.parquet':\n",
    "                        df=df.rename({col:col+file[4:]})        \n",
    "            base=base.join(df, on='case_id', how='left')\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files/train/train_base.parquet\n",
      "parquet_files/train/train_person_1.parquet\n",
      "parquet_files/train/train_person_2.parquet\n",
      "parquet_files/train/train_applprev_1_0.parquet\n",
      "parquet_files/train/train_applprev_1_1.parquet\n",
      "parquet_files/train/train_applprev_2.parquet\n",
      "parquet_files/train/train_credit_bureau_a_1_0.parquet\n",
      "parquet_files/train/train_credit_bureau_a_1_1.parquet\n",
      "parquet_files/train/train_credit_bureau_a_1_2.parquet\n",
      "parquet_files/train/train_credit_bureau_a_1_3.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_0.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_1.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_10.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_2.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_3.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_4.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_5.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_6.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_7.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_8.parquet\n",
      "parquet_files/train/train_credit_bureau_a_2_9.parquet\n",
      "parquet_files/train/train_credit_bureau_b_1.parquet\n",
      "parquet_files/train/train_credit_bureau_b_2.parquet\n",
      "parquet_files/train/train_debitcard_1.parquet\n",
      "parquet_files/train/train_deposit_1.parquet\n",
      "parquet_files/train/train_other_1.parquet\n",
      "parquet_files/train/train_static_0_0.parquet\n",
      "parquet_files/train/train_static_0_1.parquet\n",
      "parquet_files/train/train_static_cb_0.parquet\n",
      "parquet_files/train/train_tax_registry_a_1.parquet\n",
      "parquet_files/train/train_tax_registry_b_1.parquet\n",
      "parquet_files/train/train_tax_registry_c_1.parquet\n"
     ]
    }
   ],
   "source": [
    "df=load_data(files, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cols(df):\n",
    "    hashes=[df[col].hash().to_numpy().tobytes() for col in df.columns]\n",
    "    seen_hashes=[]\n",
    "    duplicated_cols=[]\n",
    "    for i in range(len(hashes)):\n",
    "        if hashes[i] not in seen_hashes:\n",
    "            seen_hashes.append(hashes[i])\n",
    "        else:\n",
    "            duplicated_cols.append(df.columns[i])\n",
    "    print(f'number of duplicated columns is {len(duplicated_cols)}')\n",
    "    df=df.drop(duplicated_cols)\n",
    "\n",
    "    constant_cols=[]\n",
    "    for col in df.columns:\n",
    "        if df[col].n_unique()<=1:\n",
    "            constant_cols.append(col)\n",
    "    print(f'number of constant columns is {len(constant_cols)}')        \n",
    "    df=df.drop(constant_cols)\n",
    "\n",
    "    high_cardinality_cols=[]\n",
    "    for col in df.columns:\n",
    "        if (df[col].n_unique()>50)&(df[col].dtype==pl.String):\n",
    "            high_cardinality_cols.append(col)\n",
    "    print(f'number of high_cardinality columns is {len(high_cardinality_cols)}')        \n",
    "    df=df.drop(high_cardinality_cols)\n",
    "\n",
    "    high_null_cols=[]\n",
    "    for col in df.columns:\n",
    "        if df[col].null_count()/len(df)>0.95:\n",
    "            high_null_cols.append(col)\n",
    "    print(f'number of high null columns is {len(high_null_cols)}')        \n",
    "    df=df.drop(high_null_cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicated columns is 57\n",
      "number of constant columns is 3\n",
      "number of high_cardinality columns is 58\n",
      "number of high null columns is 237\n"
     ]
    }
   ],
   "source": [
    "df=filter_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    for col in df.columns:\n",
    "        if str(df[col].dtype)[:3]=='Int':\n",
    "            max_=df[col].max()\n",
    "            min_=df[col].min()\n",
    "            if (min_>np.iinfo(np.int8).min) and (max_<np.iinfo(np.int8).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.Int8))\n",
    "            elif (min_>np.iinfo(np.int16).min) and (max_<np.iinfo(np.int16).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.Int16))\n",
    "            elif (min_>np.iinfo(np.int32).min) and (max_<np.iinfo(np.int32).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif (min_>np.iinfo(np.int64).min) and (max_<np.iinfo(np.int64).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "        if str(df[col].dtype)[:3]=='UIn':\n",
    "            max_=df[col].max()\n",
    "            min_=df[col].min()\n",
    "            if (min_>np.iinfo(np.uint8).min) and (max_<np.iinfo(np.uint8).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.UInt8))\n",
    "            elif (min_>np.iinfo(np.uint16).min) and (max_<np.iinfo(np.uint16).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.UInt16))\n",
    "            elif (min_>np.iinfo(np.uint32).min) and (max_<np.iinfo(np.uint32).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.UInt32))\n",
    "        if str(df[col].dtype)[:3]=='Flo':\n",
    "            max_=df[col].max()\n",
    "            min_=df[col].min()\n",
    "            if(min_>np.finfo(np.float32).min) and (max_<np.finfo(np.float32).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "            elif(min_>np.finfo(np.float64).min) and (max_<np.finfo(np.float64).max):\n",
    "                df=df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "                print(col)\n",
    "        if df[col].dtype==pl.Datetime:\n",
    "            df=df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=optimize_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def impute_na(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype==pl.String:\n",
    "            df=df.with_columns(pl.col(col).fill_null('None'))\n",
    "        elif df[col].dtype==pl.Date:\n",
    "            df=df.with_columns(pl.col(col).fill_null(df[col].median()))\n",
    "        else:\n",
    "            df=df.with_columns(pl.col(col).fill_null(df[col].mean()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=impute_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dates(df):\n",
    "    for col in df.columns:\n",
    "        if col[-1]=='D':\n",
    "            df=df.with_columns((pl.col(col)-pl.col('date_decision')).dt.total_days())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=handle_dates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df):\n",
    "    df=df.with_columns(\n",
    "        month_decision=pl.col('date_decision').dt.month(),\n",
    "        weekday_decision=pl.col('date_decision').dt.weekday()        \n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=feature_eng(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('case_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('date_decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=df['target']\n",
    "df=df.drop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=target.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=df['WEEK_NUM']>=78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09937189640908677"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[mask]\n",
    "y=target[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=X['WEEK_NUM'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=[]\n",
    "y_dev=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for group in groups:\n",
    "    X_inter=X[X['WEEK_NUM']==group]\n",
    "    y_inter=y[X['WEEK_NUM']==group]\n",
    "    X_inter_dev, X_inter_test, y_inter_dev, y_inter_test=train_test_split(X_inter, y_inter, test_size=0.5, random_state=0, stratify=y_inter)\n",
    "    X_dev.append(X_inter_dev)\n",
    "    X_test.append(X_inter_test)\n",
    "    y_dev.append(y_inter_dev)\n",
    "    y_test.append(y_inter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=pd.concat(X_dev)\n",
    "X_test=pd.concat(X_test)\n",
    "y_dev=pd.concat(y_dev)\n",
    "y_test=pd.concat(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~mask]\n",
    "target=target[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=[col for col in df.columns if df[col].dtype=='O' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=TargetEncoder(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols]=encoder.fit_transform(df[cat_cols], target)\n",
    "X_dev[cat_cols]=encoder.transform(X_dev[cat_cols])\n",
    "X_test[cat_cols]=encoder.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols]=encoder.fit_transform(df[cat_cols])\n",
    "X_dev[cat_cols]=encoder.transform(X_dev[cat_cols])\n",
    "X_test[cat_cols]=encoder.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder,f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df.columns:\n",
    "#     if df[col].dtype=='O':\n",
    "#         df[col]=df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    for col in df.columns:\n",
    "        if str(df[col].dtype)[:3]=='int':\n",
    "            max_=df[col].max()\n",
    "            min_=df[col].min()\n",
    "            if (min_>np.iinfo(np.int8).min) and (max_<np.iinfo(np.int8).max):\n",
    "                df[col]=df[col].astype('int8')\n",
    "            elif (min_>np.iinfo(np.int16).min) and (max_<np.iinfo(np.int16).max):\n",
    "                df[col]=df[col].astype('int16')\n",
    "            elif (min_>np.iinfo(np.int32).min) and (max_<np.iinfo(np.int32).max):\n",
    "                df[col]=df[col].astype('int32')\n",
    "            elif (min_>np.iinfo(np.int64).min) and (max_<np.iinfo(np.int64).max):\n",
    "                df[col]=df[col].astype('int64')\n",
    "        if str(df[col].dtype)[:3]=='flo':\n",
    "            max_=df[col].max()\n",
    "            min_=df[col].min()\n",
    "            if(min_>np.finfo(np.float16).min) and (max_<np.finfo(np.float16).max):\n",
    "                df[col]=df[col].astype('float16')\n",
    "            elif(min_>np.finfo(np.float32).min) and (max_<np.finfo(np.float32).max):\n",
    "                df[col]=df[col].astype('float32')\n",
    "            elif(min_>np.finfo(np.float64).min) and (max_<np.finfo(np.float64).max):\n",
    "                df[col]=df[col].astype('float64')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=optimize_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weeks=df['WEEK_NUM']\n",
    "dev_weeks=X_dev['WEEK_NUM']\n",
    "test_weeks=X_test['WEEK_NUM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols=[col for col in df.columns if col not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols]=scaler.fit_transform(df[num_cols])\n",
    "X_dev[num_cols]=scaler.transform(X_dev[num_cols])\n",
    "X_test[num_cols]=scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 03:48:16.223625: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-08 03:48:16.231926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746676096.238936    1214 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746676096.240987    1214 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746676096.246592    1214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746676096.246599    1214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746676096.246601    1214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746676096.246601    1214 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 03:48:16.248869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "model=tf.keras.Sequential([tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['auc'])\n",
    "model.fit(df, target, \n",
    "      epochs=100, \n",
    "      batch_size=512, \n",
    "      validation_data=(X_dev, y_dev), \n",
    "      callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, X_test):\n",
    "    y_preds=[]\n",
    "    for model in models:\n",
    "        if isinstance(model, xgb.Booster):\n",
    "            xgb_test=xgb.DMatrix(X_test, enable_categorical=True)\n",
    "            y_pred=model.predict(xgb_test)\n",
    "        elif isinstance(model, catboost.CatBoost):\n",
    "            y_pred=model.predict_proba(X_test)[:,1]\n",
    "        elif isinstance(model, tf.keras.Sequential):\n",
    "            y_pred=model.predict(X_test).ravel()\n",
    "        elif isinstance(model, lgb.Booster):\n",
    "            y_pred=model.predict(X_test)\n",
    "        else:\n",
    "                y_pred=model.predict_proba(X_test)[:,1]\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_metric(model, X_test, y_test, test_week):\n",
    "    weeks=[]\n",
    "    ginis=[]\n",
    "    for week in sorted(test_week.unique()):\n",
    "        mask=test_week==week\n",
    "        y_pred=predict(model, X_test[mask])\n",
    "        y_true=y_test[mask]\n",
    "        if len(np.unique(y_true)) < 2:  \n",
    "            print(f\"Skipping week {week}: Only one class in y_true.\")\n",
    "            continue\n",
    "        gini=2*roc_auc_score(y_true, y_pred)-1\n",
    "        weeks.append(week)\n",
    "        ginis.append(gini)\n",
    "    slope,intercept,_,_,_=linregress(weeks, ginis)            \n",
    "    std=np.std([slope*week+intercept-gini for week, gini in zip(weeks, ginis)])\n",
    "    final_score= np.mean(ginis)+88*min(0,slope)-0.5*std\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_metric([model], X_dev, y_dev, dev_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic,_ =f_classif(df, target)\n",
    "f_dict={key: value for key, value in zip(df.columns, f_statistic) if not np.isnan(value)}\n",
    "f_dict=dict(sorted(f_dict.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for threshold in [0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    n_features= round(threshold*len(f_dict))\n",
    "    chosen_cols=list(f_dict.keys())[:n_features]\n",
    "    early_stopping=tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "    model=tf.keras.Sequential([tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['auc'])\n",
    "    model.fit(df[chosen_cols], target, \n",
    "      epochs=100, \n",
    "      batch_size=512, \n",
    "      validation_data=(X_dev[chosen_cols], y_dev), \n",
    "      callbacks=[early_stopping])\n",
    "    print(threshold, stability_metric([model], X_dev[chosen_cols], y_dev, dev_weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi=mutual_info_classif(df, target, n_jobs=16)\n",
    "mi_dict={key: value for key, value in zip(df.columns, mi) if not np.isnan(value)}\n",
    "mi_dict=dict(sorted(mi_dict.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for threshold in [0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    n_features= round(threshold*len(mi_dict))\n",
    "    chosen_cols=list(mi_dict.keys())[:n_features]\n",
    "    early_stopping=tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "    model=tf.keras.Sequential([tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['auc'])\n",
    "    model.fit(df[chosen_cols], target, \n",
    "      epochs=100, \n",
    "      batch_size=512, \n",
    "      validation_data=(X_dev[chosen_cols], y_dev), \n",
    "      callbacks=[early_stopping])\n",
    "    print(threshold, stability_metric([model], X_dev[chosen_cols], y_dev, dev_weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chosen_cols)==len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "model=tf.keras.Sequential([tf.keras.layers.Dense(50, activation='sigmoid'),\n",
    "                          tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['auc'])\n",
    "model.fit(df, target, \n",
    "  epochs=100, \n",
    "  batch_size=512, \n",
    "  validation_data=(X_dev, y_dev), \n",
    "  callbacks=[early_stopping])\n",
    "print(stability_metric([model], X_dev, y_dev, dev_weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 44779, number of negative: 1330173\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.850916\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.386258\n",
      "[LightGBM] [Debug] init for col-wise cost 0.140942 seconds, init for row-wise cost 1.872083 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.977118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87608\n",
      "[LightGBM] [Info] Number of data points in the train set: 1374952, number of used features: 737\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032568 -> initscore=-3.391325\n",
      "[LightGBM] [Info] Start training from score -3.391325\n",
      "[LightGBM] [Debug] Re-bagging, using 1099036 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100201 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099557 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1100157 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099750 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099462 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1100148 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1100193 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100223 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099941 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100586 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1100107 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099651 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099818 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099884 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099728 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099902 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099935 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099116 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099530 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1100035 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099968 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099722 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1100668 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100039 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100191 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099875 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1098798 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1098858 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099950 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099740 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1100417 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100040 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100324 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1100403 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099998 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100279 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099871 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100121 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 1099824 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099880 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099959 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100259 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1100211 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100062 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099898 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100116 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099843 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100642 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099799 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1098765 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100251 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100086 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099546 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100663 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100060 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099854 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100440 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100095 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099823 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100094 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100198 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100075 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1100388 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100207 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100694 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099920 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1098334 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100889 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100238 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1101015 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100024 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100519 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099398 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100144 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099821 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099261 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100194 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100888 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099038 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100246 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100545 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099835 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100053 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100117 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099708 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099153 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100551 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1098830 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099696 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099949 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100413 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100520 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100250 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099290 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099932 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100327 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099880 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099344 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100667 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099744 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100263 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099727 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100539 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099896 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099948 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100022 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100075 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100314 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100273 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099860 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100761 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099882 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099576 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Re-bagging, using 1099256 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1098898 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099363 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100670 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099430 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099877 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099911 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099964 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099415 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100747 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099859 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100170 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100182 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100584 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099416 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099463 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099876 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099171 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099665 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100426 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099939 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099549 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100334 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099887 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099725 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 1100427 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1098860 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099642 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100431 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099697 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100057 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099970 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099673 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 1099819 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 19\n",
      "[LightGBM] [Debug] Re-bagging, using 1099490 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100292 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100012 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100577 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100068 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100273 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100128 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099892 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099673 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099369 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100099 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1098231 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100654 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 20\n",
      "[LightGBM] [Debug] Re-bagging, using 1099912 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099792 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100326 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 22\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Re-bagging, using 1098763 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1101147 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100034 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099051 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099655 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Re-bagging, using 1100508 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099914 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099821 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099513 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100122 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100355 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099675 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099732 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099952 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100510 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099328 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099766 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100454 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099998 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099214 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100209 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099842 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100085 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100178 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099815 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 22\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099496 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1098518 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1097706 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1098181 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099900 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099882 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1099828 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 1099675 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100467 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100561 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100353 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100116 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100437 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100639 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1099473 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1099583 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100378 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099099 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100253 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099835 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1098770 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100496 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100210 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099999 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099862 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1099958 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099566 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 19\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100862 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100832 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100228 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099832 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099340 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100033 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 1099614 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 18\n",
      "[LightGBM] [Debug] Re-bagging, using 1100172 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100112 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 21\n",
      "[LightGBM] [Debug] Re-bagging, using 1100916 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100182 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100118 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099942 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100628 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100616 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099816 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100377 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099275 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100497 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100020 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100154 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1099405 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100048 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100555 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 1100130 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100600 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100415 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1098587 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099762 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099810 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 1100425 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Re-bagging, using 1099788 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1100251 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100108 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100336 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 23\n",
      "[LightGBM] [Debug] Re-bagging, using 1100000 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099592 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100212 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099782 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1099425 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 10\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1099186 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099921 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099736 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1100228 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 9\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1099121 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 16\n",
      "[LightGBM] [Debug] Re-bagging, using 1099394 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1100237 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099931 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 1100442 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 17\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100242 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100186 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 1100245 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 1099058 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 1100360 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 20\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 1099473 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 41 and depth = 14\n",
      "Early stopping, best iteration is:\n",
      "[531]\tvalid_0's auc: 0.876121\n"
     ]
    }
   ],
   "source": [
    "lgb_train=lgb.Dataset(df, target)\n",
    "lgb_valid=lgb.Dataset(X_dev, y_dev, reference=lgb_train)\n",
    "params={'objective':'binary', \n",
    "                'metrics':'auc',\n",
    "        'n_estimators':1000,\n",
    "        'num_leaves':41,\n",
    "        'learning_rate':0.03,\n",
    "        'colsample_bytree':0.8,\n",
    "        'colsample_bynode':0.8,\n",
    "        'bagging_fraction':0.8,\n",
    "            'bagging_freq':2,\n",
    "                 'seed':0,\n",
    "                'device_type':'cpu',\n",
    "       'verbose':2}\n",
    "lgb_model=lgb.train(params,\n",
    "     lgb_train,\n",
    "     valid_sets=lgb_valid,\n",
    "     callbacks=[lgb.early_stopping(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7301301558075108)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_metric([lgb_model], X_dev, y_dev, dev_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\txgb_valid-auc:0.76278\n",
      "[1]\txgb_valid-auc:0.78701\n",
      "[2]\txgb_valid-auc:0.79545\n",
      "[3]\txgb_valid-auc:0.80625\n",
      "[4]\txgb_valid-auc:0.80927\n",
      "[5]\txgb_valid-auc:0.81343\n",
      "[6]\txgb_valid-auc:0.81681\n",
      "[7]\txgb_valid-auc:0.81916\n",
      "[8]\txgb_valid-auc:0.82122\n",
      "[9]\txgb_valid-auc:0.82235\n",
      "[10]\txgb_valid-auc:0.82285\n",
      "[11]\txgb_valid-auc:0.82377\n",
      "[12]\txgb_valid-auc:0.82374\n",
      "[13]\txgb_valid-auc:0.82508\n",
      "[14]\txgb_valid-auc:0.82638\n",
      "[15]\txgb_valid-auc:0.82824\n",
      "[16]\txgb_valid-auc:0.82934\n",
      "[17]\txgb_valid-auc:0.82958\n",
      "[18]\txgb_valid-auc:0.83152\n",
      "[19]\txgb_valid-auc:0.83217\n",
      "[20]\txgb_valid-auc:0.83308\n",
      "[21]\txgb_valid-auc:0.83453\n",
      "[22]\txgb_valid-auc:0.83618\n",
      "[23]\txgb_valid-auc:0.83768\n",
      "[24]\txgb_valid-auc:0.83837\n",
      "[25]\txgb_valid-auc:0.83907\n",
      "[26]\txgb_valid-auc:0.83972\n",
      "[27]\txgb_valid-auc:0.84032\n",
      "[28]\txgb_valid-auc:0.84120\n",
      "[29]\txgb_valid-auc:0.84164\n",
      "[30]\txgb_valid-auc:0.84239\n",
      "[31]\txgb_valid-auc:0.84316\n",
      "[32]\txgb_valid-auc:0.84399\n",
      "[33]\txgb_valid-auc:0.84449\n",
      "[34]\txgb_valid-auc:0.84510\n",
      "[35]\txgb_valid-auc:0.84565\n",
      "[36]\txgb_valid-auc:0.84626\n",
      "[37]\txgb_valid-auc:0.84686\n",
      "[38]\txgb_valid-auc:0.84751\n",
      "[39]\txgb_valid-auc:0.84770\n",
      "[40]\txgb_valid-auc:0.84793\n",
      "[41]\txgb_valid-auc:0.84802\n",
      "[42]\txgb_valid-auc:0.84862\n",
      "[43]\txgb_valid-auc:0.84900\n",
      "[44]\txgb_valid-auc:0.84924\n",
      "[45]\txgb_valid-auc:0.84971\n",
      "[46]\txgb_valid-auc:0.85016\n",
      "[47]\txgb_valid-auc:0.85059\n",
      "[48]\txgb_valid-auc:0.85102\n",
      "[49]\txgb_valid-auc:0.85158\n",
      "[50]\txgb_valid-auc:0.85207\n",
      "[51]\txgb_valid-auc:0.85234\n",
      "[52]\txgb_valid-auc:0.85252\n",
      "[53]\txgb_valid-auc:0.85282\n",
      "[54]\txgb_valid-auc:0.85296\n",
      "[55]\txgb_valid-auc:0.85339\n",
      "[56]\txgb_valid-auc:0.85340\n",
      "[57]\txgb_valid-auc:0.85354\n",
      "[58]\txgb_valid-auc:0.85370\n",
      "[59]\txgb_valid-auc:0.85433\n",
      "[60]\txgb_valid-auc:0.85455\n",
      "[61]\txgb_valid-auc:0.85496\n",
      "[62]\txgb_valid-auc:0.85511\n",
      "[63]\txgb_valid-auc:0.85523\n",
      "[64]\txgb_valid-auc:0.85558\n",
      "[65]\txgb_valid-auc:0.85595\n",
      "[66]\txgb_valid-auc:0.85618\n",
      "[67]\txgb_valid-auc:0.85635\n",
      "[68]\txgb_valid-auc:0.85682\n",
      "[69]\txgb_valid-auc:0.85721\n",
      "[70]\txgb_valid-auc:0.85733\n",
      "[71]\txgb_valid-auc:0.85757\n",
      "[72]\txgb_valid-auc:0.85765\n",
      "[73]\txgb_valid-auc:0.85792\n",
      "[74]\txgb_valid-auc:0.85819\n",
      "[75]\txgb_valid-auc:0.85832\n",
      "[76]\txgb_valid-auc:0.85850\n",
      "[77]\txgb_valid-auc:0.85865\n",
      "[78]\txgb_valid-auc:0.85871\n",
      "[79]\txgb_valid-auc:0.85890\n",
      "[80]\txgb_valid-auc:0.85925\n",
      "[81]\txgb_valid-auc:0.85946\n",
      "[82]\txgb_valid-auc:0.85953\n",
      "[83]\txgb_valid-auc:0.85964\n",
      "[84]\txgb_valid-auc:0.85984\n",
      "[85]\txgb_valid-auc:0.86006\n",
      "[86]\txgb_valid-auc:0.86028\n",
      "[87]\txgb_valid-auc:0.86046\n",
      "[88]\txgb_valid-auc:0.86069\n",
      "[89]\txgb_valid-auc:0.86086\n",
      "[90]\txgb_valid-auc:0.86109\n",
      "[91]\txgb_valid-auc:0.86114\n",
      "[92]\txgb_valid-auc:0.86127\n",
      "[93]\txgb_valid-auc:0.86138\n",
      "[94]\txgb_valid-auc:0.86143\n",
      "[95]\txgb_valid-auc:0.86154\n",
      "[96]\txgb_valid-auc:0.86169\n",
      "[97]\txgb_valid-auc:0.86199\n",
      "[98]\txgb_valid-auc:0.86211\n",
      "[99]\txgb_valid-auc:0.86226\n",
      "[100]\txgb_valid-auc:0.86250\n",
      "[101]\txgb_valid-auc:0.86258\n",
      "[102]\txgb_valid-auc:0.86263\n",
      "[103]\txgb_valid-auc:0.86270\n",
      "[104]\txgb_valid-auc:0.86276\n",
      "[105]\txgb_valid-auc:0.86286\n",
      "[106]\txgb_valid-auc:0.86314\n",
      "[107]\txgb_valid-auc:0.86346\n",
      "[108]\txgb_valid-auc:0.86369\n",
      "[109]\txgb_valid-auc:0.86392\n",
      "[110]\txgb_valid-auc:0.86397\n",
      "[111]\txgb_valid-auc:0.86416\n",
      "[112]\txgb_valid-auc:0.86426\n",
      "[113]\txgb_valid-auc:0.86440\n",
      "[114]\txgb_valid-auc:0.86467\n",
      "[115]\txgb_valid-auc:0.86488\n",
      "[116]\txgb_valid-auc:0.86486\n",
      "[117]\txgb_valid-auc:0.86498\n",
      "[118]\txgb_valid-auc:0.86506\n",
      "[119]\txgb_valid-auc:0.86519\n",
      "[120]\txgb_valid-auc:0.86532\n",
      "[121]\txgb_valid-auc:0.86554\n",
      "[122]\txgb_valid-auc:0.86566\n",
      "[123]\txgb_valid-auc:0.86590\n",
      "[124]\txgb_valid-auc:0.86609\n",
      "[125]\txgb_valid-auc:0.86650\n",
      "[126]\txgb_valid-auc:0.86661\n",
      "[127]\txgb_valid-auc:0.86662\n",
      "[128]\txgb_valid-auc:0.86676\n",
      "[129]\txgb_valid-auc:0.86692\n",
      "[130]\txgb_valid-auc:0.86701\n",
      "[131]\txgb_valid-auc:0.86710\n",
      "[132]\txgb_valid-auc:0.86713\n",
      "[133]\txgb_valid-auc:0.86721\n",
      "[134]\txgb_valid-auc:0.86720\n",
      "[135]\txgb_valid-auc:0.86735\n",
      "[136]\txgb_valid-auc:0.86742\n",
      "[137]\txgb_valid-auc:0.86756\n",
      "[138]\txgb_valid-auc:0.86774\n",
      "[139]\txgb_valid-auc:0.86777\n",
      "[140]\txgb_valid-auc:0.86805\n",
      "[141]\txgb_valid-auc:0.86830\n",
      "[142]\txgb_valid-auc:0.86842\n",
      "[143]\txgb_valid-auc:0.86860\n",
      "[144]\txgb_valid-auc:0.86858\n",
      "[145]\txgb_valid-auc:0.86865\n",
      "[146]\txgb_valid-auc:0.86875\n",
      "[147]\txgb_valid-auc:0.86898\n",
      "[148]\txgb_valid-auc:0.86898\n",
      "[149]\txgb_valid-auc:0.86896\n",
      "[150]\txgb_valid-auc:0.86901\n",
      "[151]\txgb_valid-auc:0.86909\n",
      "[152]\txgb_valid-auc:0.86913\n",
      "[153]\txgb_valid-auc:0.86926\n",
      "[154]\txgb_valid-auc:0.86925\n",
      "[155]\txgb_valid-auc:0.86924\n",
      "[156]\txgb_valid-auc:0.86940\n",
      "[157]\txgb_valid-auc:0.86941\n",
      "[158]\txgb_valid-auc:0.86949\n",
      "[159]\txgb_valid-auc:0.86962\n",
      "[160]\txgb_valid-auc:0.86972\n",
      "[161]\txgb_valid-auc:0.86972\n",
      "[162]\txgb_valid-auc:0.86984\n",
      "[163]\txgb_valid-auc:0.86994\n",
      "[164]\txgb_valid-auc:0.87007\n",
      "[165]\txgb_valid-auc:0.87013\n",
      "[166]\txgb_valid-auc:0.87029\n",
      "[167]\txgb_valid-auc:0.87026\n",
      "[168]\txgb_valid-auc:0.87028\n",
      "[169]\txgb_valid-auc:0.87033\n",
      "[170]\txgb_valid-auc:0.87031\n",
      "[171]\txgb_valid-auc:0.87037\n",
      "[172]\txgb_valid-auc:0.87037\n",
      "[173]\txgb_valid-auc:0.87042\n",
      "[174]\txgb_valid-auc:0.87045\n",
      "[175]\txgb_valid-auc:0.87058\n",
      "[176]\txgb_valid-auc:0.87064\n",
      "[177]\txgb_valid-auc:0.87066\n",
      "[178]\txgb_valid-auc:0.87071\n",
      "[179]\txgb_valid-auc:0.87076\n",
      "[180]\txgb_valid-auc:0.87092\n",
      "[181]\txgb_valid-auc:0.87106\n",
      "[182]\txgb_valid-auc:0.87113\n",
      "[183]\txgb_valid-auc:0.87118\n",
      "[184]\txgb_valid-auc:0.87124\n",
      "[185]\txgb_valid-auc:0.87136\n",
      "[186]\txgb_valid-auc:0.87143\n",
      "[187]\txgb_valid-auc:0.87150\n",
      "[188]\txgb_valid-auc:0.87151\n",
      "[189]\txgb_valid-auc:0.87161\n",
      "[190]\txgb_valid-auc:0.87165\n",
      "[191]\txgb_valid-auc:0.87178\n",
      "[192]\txgb_valid-auc:0.87190\n",
      "[193]\txgb_valid-auc:0.87190\n",
      "[194]\txgb_valid-auc:0.87201\n",
      "[195]\txgb_valid-auc:0.87205\n",
      "[196]\txgb_valid-auc:0.87206\n",
      "[197]\txgb_valid-auc:0.87218\n",
      "[198]\txgb_valid-auc:0.87219\n",
      "[199]\txgb_valid-auc:0.87223\n",
      "[200]\txgb_valid-auc:0.87227\n",
      "[201]\txgb_valid-auc:0.87232\n",
      "[202]\txgb_valid-auc:0.87243\n",
      "[203]\txgb_valid-auc:0.87249\n",
      "[204]\txgb_valid-auc:0.87255\n",
      "[205]\txgb_valid-auc:0.87261\n",
      "[206]\txgb_valid-auc:0.87264\n",
      "[207]\txgb_valid-auc:0.87267\n",
      "[208]\txgb_valid-auc:0.87273\n",
      "[209]\txgb_valid-auc:0.87286\n",
      "[210]\txgb_valid-auc:0.87290\n",
      "[211]\txgb_valid-auc:0.87294\n",
      "[212]\txgb_valid-auc:0.87302\n",
      "[213]\txgb_valid-auc:0.87313\n",
      "[214]\txgb_valid-auc:0.87320\n",
      "[215]\txgb_valid-auc:0.87325\n",
      "[216]\txgb_valid-auc:0.87326\n",
      "[217]\txgb_valid-auc:0.87319\n",
      "[218]\txgb_valid-auc:0.87320\n",
      "[219]\txgb_valid-auc:0.87322\n",
      "[220]\txgb_valid-auc:0.87333\n",
      "[221]\txgb_valid-auc:0.87339\n",
      "[222]\txgb_valid-auc:0.87341\n",
      "[223]\txgb_valid-auc:0.87341\n",
      "[224]\txgb_valid-auc:0.87342\n",
      "[225]\txgb_valid-auc:0.87349\n",
      "[226]\txgb_valid-auc:0.87349\n",
      "[227]\txgb_valid-auc:0.87348\n",
      "[228]\txgb_valid-auc:0.87347\n",
      "[229]\txgb_valid-auc:0.87346\n",
      "[230]\txgb_valid-auc:0.87346\n",
      "[231]\txgb_valid-auc:0.87347\n",
      "[232]\txgb_valid-auc:0.87352\n",
      "[233]\txgb_valid-auc:0.87358\n",
      "[234]\txgb_valid-auc:0.87370\n",
      "[235]\txgb_valid-auc:0.87372\n",
      "[236]\txgb_valid-auc:0.87376\n",
      "[237]\txgb_valid-auc:0.87377\n",
      "[238]\txgb_valid-auc:0.87374\n",
      "[239]\txgb_valid-auc:0.87379\n",
      "[240]\txgb_valid-auc:0.87376\n",
      "[241]\txgb_valid-auc:0.87378\n",
      "[242]\txgb_valid-auc:0.87378\n",
      "[243]\txgb_valid-auc:0.87382\n",
      "[244]\txgb_valid-auc:0.87385\n",
      "[245]\txgb_valid-auc:0.87388\n",
      "[246]\txgb_valid-auc:0.87390\n",
      "[247]\txgb_valid-auc:0.87392\n",
      "[248]\txgb_valid-auc:0.87390\n",
      "[249]\txgb_valid-auc:0.87392\n",
      "[250]\txgb_valid-auc:0.87395\n",
      "[251]\txgb_valid-auc:0.87415\n",
      "[252]\txgb_valid-auc:0.87418\n",
      "[253]\txgb_valid-auc:0.87418\n",
      "[254]\txgb_valid-auc:0.87428\n",
      "[255]\txgb_valid-auc:0.87435\n",
      "[256]\txgb_valid-auc:0.87438\n",
      "[257]\txgb_valid-auc:0.87447\n",
      "[258]\txgb_valid-auc:0.87448\n",
      "[259]\txgb_valid-auc:0.87456\n",
      "[260]\txgb_valid-auc:0.87457\n",
      "[261]\txgb_valid-auc:0.87458\n",
      "[262]\txgb_valid-auc:0.87462\n",
      "[263]\txgb_valid-auc:0.87467\n",
      "[264]\txgb_valid-auc:0.87477\n",
      "[265]\txgb_valid-auc:0.87480\n",
      "[266]\txgb_valid-auc:0.87487\n",
      "[267]\txgb_valid-auc:0.87490\n",
      "[268]\txgb_valid-auc:0.87489\n",
      "[269]\txgb_valid-auc:0.87490\n",
      "[270]\txgb_valid-auc:0.87486\n",
      "[271]\txgb_valid-auc:0.87489\n",
      "[272]\txgb_valid-auc:0.87489\n",
      "[273]\txgb_valid-auc:0.87494\n",
      "[274]\txgb_valid-auc:0.87492\n",
      "[275]\txgb_valid-auc:0.87488\n",
      "[276]\txgb_valid-auc:0.87492\n",
      "[277]\txgb_valid-auc:0.87496\n",
      "[278]\txgb_valid-auc:0.87499\n",
      "[279]\txgb_valid-auc:0.87507\n",
      "[280]\txgb_valid-auc:0.87522\n",
      "[281]\txgb_valid-auc:0.87526\n",
      "[282]\txgb_valid-auc:0.87529\n",
      "[283]\txgb_valid-auc:0.87535\n",
      "[284]\txgb_valid-auc:0.87538\n",
      "[285]\txgb_valid-auc:0.87537\n",
      "[286]\txgb_valid-auc:0.87537\n",
      "[287]\txgb_valid-auc:0.87541\n",
      "[288]\txgb_valid-auc:0.87541\n",
      "[289]\txgb_valid-auc:0.87540\n",
      "[290]\txgb_valid-auc:0.87542\n",
      "[291]\txgb_valid-auc:0.87552\n",
      "[292]\txgb_valid-auc:0.87559\n",
      "[293]\txgb_valid-auc:0.87557\n",
      "[294]\txgb_valid-auc:0.87559\n",
      "[295]\txgb_valid-auc:0.87558\n",
      "[296]\txgb_valid-auc:0.87558\n",
      "[297]\txgb_valid-auc:0.87562\n",
      "[298]\txgb_valid-auc:0.87566\n",
      "[299]\txgb_valid-auc:0.87571\n",
      "[300]\txgb_valid-auc:0.87575\n",
      "[301]\txgb_valid-auc:0.87577\n",
      "[302]\txgb_valid-auc:0.87577\n",
      "[303]\txgb_valid-auc:0.87581\n",
      "[304]\txgb_valid-auc:0.87590\n",
      "[305]\txgb_valid-auc:0.87590\n",
      "[306]\txgb_valid-auc:0.87590\n",
      "[307]\txgb_valid-auc:0.87591\n",
      "[308]\txgb_valid-auc:0.87599\n",
      "[309]\txgb_valid-auc:0.87602\n",
      "[310]\txgb_valid-auc:0.87597\n",
      "[311]\txgb_valid-auc:0.87594\n",
      "[312]\txgb_valid-auc:0.87592\n",
      "[313]\txgb_valid-auc:0.87590\n",
      "[314]\txgb_valid-auc:0.87589\n",
      "[315]\txgb_valid-auc:0.87593\n",
      "[316]\txgb_valid-auc:0.87594\n",
      "[317]\txgb_valid-auc:0.87607\n",
      "[318]\txgb_valid-auc:0.87598\n",
      "[319]\txgb_valid-auc:0.87593\n",
      "[320]\txgb_valid-auc:0.87595\n",
      "[321]\txgb_valid-auc:0.87593\n",
      "[322]\txgb_valid-auc:0.87593\n",
      "[323]\txgb_valid-auc:0.87598\n",
      "[324]\txgb_valid-auc:0.87604\n",
      "[325]\txgb_valid-auc:0.87611\n",
      "[326]\txgb_valid-auc:0.87623\n",
      "[327]\txgb_valid-auc:0.87629\n",
      "[328]\txgb_valid-auc:0.87630\n",
      "[329]\txgb_valid-auc:0.87632\n",
      "[330]\txgb_valid-auc:0.87630\n",
      "[331]\txgb_valid-auc:0.87627\n",
      "[332]\txgb_valid-auc:0.87628\n",
      "[333]\txgb_valid-auc:0.87630\n",
      "[334]\txgb_valid-auc:0.87629\n",
      "[335]\txgb_valid-auc:0.87635\n",
      "[336]\txgb_valid-auc:0.87639\n",
      "[337]\txgb_valid-auc:0.87651\n",
      "[338]\txgb_valid-auc:0.87655\n",
      "[339]\txgb_valid-auc:0.87658\n",
      "[340]\txgb_valid-auc:0.87657\n",
      "[341]\txgb_valid-auc:0.87661\n",
      "[342]\txgb_valid-auc:0.87665\n",
      "[343]\txgb_valid-auc:0.87662\n",
      "[344]\txgb_valid-auc:0.87662\n",
      "[345]\txgb_valid-auc:0.87660\n",
      "[346]\txgb_valid-auc:0.87660\n",
      "[347]\txgb_valid-auc:0.87673\n",
      "[348]\txgb_valid-auc:0.87677\n",
      "[349]\txgb_valid-auc:0.87683\n",
      "[350]\txgb_valid-auc:0.87691\n",
      "[351]\txgb_valid-auc:0.87692\n",
      "[352]\txgb_valid-auc:0.87693\n",
      "[353]\txgb_valid-auc:0.87694\n",
      "[354]\txgb_valid-auc:0.87698\n",
      "[355]\txgb_valid-auc:0.87694\n",
      "[356]\txgb_valid-auc:0.87696\n",
      "[357]\txgb_valid-auc:0.87697\n",
      "[358]\txgb_valid-auc:0.87694\n",
      "[359]\txgb_valid-auc:0.87701\n",
      "[360]\txgb_valid-auc:0.87708\n",
      "[361]\txgb_valid-auc:0.87712\n",
      "[362]\txgb_valid-auc:0.87708\n",
      "[363]\txgb_valid-auc:0.87710\n",
      "[364]\txgb_valid-auc:0.87712\n",
      "[365]\txgb_valid-auc:0.87715\n",
      "[366]\txgb_valid-auc:0.87717\n",
      "[367]\txgb_valid-auc:0.87721\n",
      "[368]\txgb_valid-auc:0.87726\n",
      "[369]\txgb_valid-auc:0.87729\n",
      "[370]\txgb_valid-auc:0.87729\n",
      "[371]\txgb_valid-auc:0.87729\n",
      "[372]\txgb_valid-auc:0.87730\n",
      "[373]\txgb_valid-auc:0.87727\n",
      "[374]\txgb_valid-auc:0.87736\n",
      "[375]\txgb_valid-auc:0.87735\n",
      "[376]\txgb_valid-auc:0.87743\n",
      "[377]\txgb_valid-auc:0.87743\n",
      "[378]\txgb_valid-auc:0.87745\n",
      "[379]\txgb_valid-auc:0.87743\n",
      "[380]\txgb_valid-auc:0.87746\n",
      "[381]\txgb_valid-auc:0.87750\n",
      "[382]\txgb_valid-auc:0.87756\n",
      "[383]\txgb_valid-auc:0.87758\n",
      "[384]\txgb_valid-auc:0.87757\n",
      "[385]\txgb_valid-auc:0.87760\n",
      "[386]\txgb_valid-auc:0.87759\n",
      "[387]\txgb_valid-auc:0.87758\n",
      "[388]\txgb_valid-auc:0.87778\n",
      "[389]\txgb_valid-auc:0.87781\n",
      "[390]\txgb_valid-auc:0.87777\n",
      "[391]\txgb_valid-auc:0.87779\n",
      "[392]\txgb_valid-auc:0.87781\n",
      "[393]\txgb_valid-auc:0.87785\n",
      "[394]\txgb_valid-auc:0.87787\n",
      "[395]\txgb_valid-auc:0.87786\n",
      "[396]\txgb_valid-auc:0.87786\n",
      "[397]\txgb_valid-auc:0.87790\n",
      "[398]\txgb_valid-auc:0.87788\n",
      "[399]\txgb_valid-auc:0.87791\n",
      "[400]\txgb_valid-auc:0.87799\n",
      "[401]\txgb_valid-auc:0.87802\n",
      "[402]\txgb_valid-auc:0.87805\n",
      "[403]\txgb_valid-auc:0.87806\n",
      "[404]\txgb_valid-auc:0.87806\n",
      "[405]\txgb_valid-auc:0.87807\n",
      "[406]\txgb_valid-auc:0.87811\n",
      "[407]\txgb_valid-auc:0.87811\n",
      "[408]\txgb_valid-auc:0.87811\n",
      "[409]\txgb_valid-auc:0.87814\n",
      "[410]\txgb_valid-auc:0.87823\n",
      "[411]\txgb_valid-auc:0.87820\n",
      "[412]\txgb_valid-auc:0.87817\n",
      "[413]\txgb_valid-auc:0.87820\n",
      "[414]\txgb_valid-auc:0.87823\n",
      "[415]\txgb_valid-auc:0.87826\n",
      "[416]\txgb_valid-auc:0.87829\n",
      "[417]\txgb_valid-auc:0.87828\n",
      "[418]\txgb_valid-auc:0.87828\n",
      "[419]\txgb_valid-auc:0.87827\n",
      "[420]\txgb_valid-auc:0.87826\n",
      "[421]\txgb_valid-auc:0.87831\n",
      "[422]\txgb_valid-auc:0.87831\n",
      "[423]\txgb_valid-auc:0.87832\n",
      "[424]\txgb_valid-auc:0.87836\n",
      "[425]\txgb_valid-auc:0.87837\n",
      "[426]\txgb_valid-auc:0.87836\n",
      "[427]\txgb_valid-auc:0.87840\n",
      "[428]\txgb_valid-auc:0.87839\n",
      "[429]\txgb_valid-auc:0.87841\n",
      "[430]\txgb_valid-auc:0.87844\n",
      "[431]\txgb_valid-auc:0.87845\n",
      "[432]\txgb_valid-auc:0.87845\n",
      "[433]\txgb_valid-auc:0.87846\n",
      "[434]\txgb_valid-auc:0.87842\n",
      "[435]\txgb_valid-auc:0.87841\n",
      "[436]\txgb_valid-auc:0.87846\n",
      "[437]\txgb_valid-auc:0.87848\n",
      "[438]\txgb_valid-auc:0.87851\n",
      "[439]\txgb_valid-auc:0.87851\n",
      "[440]\txgb_valid-auc:0.87856\n",
      "[441]\txgb_valid-auc:0.87864\n",
      "[442]\txgb_valid-auc:0.87870\n",
      "[443]\txgb_valid-auc:0.87870\n",
      "[444]\txgb_valid-auc:0.87872\n",
      "[445]\txgb_valid-auc:0.87862\n",
      "[446]\txgb_valid-auc:0.87867\n",
      "[447]\txgb_valid-auc:0.87866\n",
      "[448]\txgb_valid-auc:0.87870\n",
      "[449]\txgb_valid-auc:0.87870\n",
      "[450]\txgb_valid-auc:0.87870\n",
      "[451]\txgb_valid-auc:0.87870\n",
      "[452]\txgb_valid-auc:0.87867\n",
      "[453]\txgb_valid-auc:0.87866\n"
     ]
    }
   ],
   "source": [
    "xgb_train=xgb.DMatrix(df, target)\n",
    "xgb_valid=xgb.DMatrix(X_dev, y_dev)\n",
    "params={'objective':'binary:logistic', \n",
    "                'eval_metric':'auc',\n",
    "                 'seed':0,\n",
    "        'learning_rate':0.05,\n",
    "        'max_depth':6,\n",
    "        'colsample_bytree':0.8,\n",
    "        'colsample_bynode':0.8,\n",
    "        'subsample':0.8,\n",
    "                'device':'cpu'}\n",
    "xgb_model=xgb.train(params,\n",
    "     xgb_train,\n",
    "     evals=[(xgb_valid,'xgb_valid')],\n",
    "     num_boost_round=1000,\n",
    "     early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7377228526025422)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_metric([xgb_model], X_dev, y_dev, dev_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.6104762\tbest: 0.6104762 (0)\ttotal: 204ms\tremaining: 3m 23s\n",
      "1:\ttest: 0.6582930\tbest: 0.6582930 (1)\ttotal: 377ms\tremaining: 3m 8s\n",
      "2:\ttest: 0.6889485\tbest: 0.6889485 (2)\ttotal: 525ms\tremaining: 2m 54s\n",
      "3:\ttest: 0.7295184\tbest: 0.7295184 (3)\ttotal: 692ms\tremaining: 2m 52s\n",
      "4:\ttest: 0.7474547\tbest: 0.7474547 (4)\ttotal: 897ms\tremaining: 2m 58s\n",
      "5:\ttest: 0.7559199\tbest: 0.7559199 (5)\ttotal: 1.08s\tremaining: 2m 59s\n",
      "6:\ttest: 0.7536881\tbest: 0.7559199 (5)\ttotal: 1.28s\tremaining: 3m 1s\n",
      "7:\ttest: 0.7587315\tbest: 0.7587315 (7)\ttotal: 1.5s\tremaining: 3m 6s\n",
      "8:\ttest: 0.7673417\tbest: 0.7673417 (8)\ttotal: 1.68s\tremaining: 3m 5s\n",
      "9:\ttest: 0.7666463\tbest: 0.7673417 (8)\ttotal: 1.83s\tremaining: 3m\n",
      "10:\ttest: 0.7657717\tbest: 0.7673417 (8)\ttotal: 2.02s\tremaining: 3m 1s\n",
      "11:\ttest: 0.7690631\tbest: 0.7690631 (11)\ttotal: 2.19s\tremaining: 3m\n",
      "12:\ttest: 0.7753941\tbest: 0.7753941 (12)\ttotal: 2.37s\tremaining: 2m 59s\n",
      "13:\ttest: 0.7805817\tbest: 0.7805817 (13)\ttotal: 2.55s\tremaining: 2m 59s\n",
      "14:\ttest: 0.7857532\tbest: 0.7857532 (14)\ttotal: 2.74s\tremaining: 2m 59s\n",
      "15:\ttest: 0.7896711\tbest: 0.7896711 (15)\ttotal: 2.9s\tremaining: 2m 58s\n",
      "16:\ttest: 0.7922238\tbest: 0.7922238 (16)\ttotal: 3.09s\tremaining: 2m 58s\n",
      "17:\ttest: 0.7927178\tbest: 0.7927178 (17)\ttotal: 3.25s\tremaining: 2m 57s\n",
      "18:\ttest: 0.7953748\tbest: 0.7953748 (18)\ttotal: 3.45s\tremaining: 2m 58s\n",
      "19:\ttest: 0.7995318\tbest: 0.7995318 (19)\ttotal: 3.66s\tremaining: 2m 59s\n",
      "20:\ttest: 0.8032035\tbest: 0.8032035 (20)\ttotal: 3.85s\tremaining: 2m 59s\n",
      "21:\ttest: 0.8084282\tbest: 0.8084282 (21)\ttotal: 4.04s\tremaining: 2m 59s\n",
      "22:\ttest: 0.8105437\tbest: 0.8105437 (22)\ttotal: 4.24s\tremaining: 3m\n",
      "23:\ttest: 0.8137360\tbest: 0.8137360 (23)\ttotal: 4.44s\tremaining: 3m\n",
      "24:\ttest: 0.8152696\tbest: 0.8152696 (24)\ttotal: 4.63s\tremaining: 3m\n",
      "25:\ttest: 0.8183488\tbest: 0.8183488 (25)\ttotal: 4.83s\tremaining: 3m 1s\n",
      "26:\ttest: 0.8208314\tbest: 0.8208314 (26)\ttotal: 5.03s\tremaining: 3m 1s\n",
      "27:\ttest: 0.8234295\tbest: 0.8234295 (27)\ttotal: 5.23s\tremaining: 3m 1s\n",
      "28:\ttest: 0.8243191\tbest: 0.8243191 (28)\ttotal: 5.43s\tremaining: 3m 1s\n",
      "29:\ttest: 0.8255324\tbest: 0.8255324 (29)\ttotal: 5.63s\tremaining: 3m 2s\n",
      "30:\ttest: 0.8263889\tbest: 0.8263889 (30)\ttotal: 5.83s\tremaining: 3m 2s\n",
      "31:\ttest: 0.8275384\tbest: 0.8275384 (31)\ttotal: 6.03s\tremaining: 3m 2s\n",
      "32:\ttest: 0.8290707\tbest: 0.8290707 (32)\ttotal: 6.22s\tremaining: 3m 2s\n",
      "33:\ttest: 0.8288574\tbest: 0.8290707 (32)\ttotal: 6.41s\tremaining: 3m 2s\n",
      "34:\ttest: 0.8286575\tbest: 0.8290707 (32)\ttotal: 6.61s\tremaining: 3m 2s\n",
      "35:\ttest: 0.8294574\tbest: 0.8294574 (35)\ttotal: 6.79s\tremaining: 3m 1s\n",
      "36:\ttest: 0.8296439\tbest: 0.8296439 (36)\ttotal: 7s\tremaining: 3m 2s\n",
      "37:\ttest: 0.8308570\tbest: 0.8308570 (37)\ttotal: 7.2s\tremaining: 3m 2s\n",
      "38:\ttest: 0.8312916\tbest: 0.8312916 (38)\ttotal: 7.39s\tremaining: 3m 2s\n",
      "39:\ttest: 0.8322280\tbest: 0.8322280 (39)\ttotal: 7.59s\tremaining: 3m 2s\n",
      "40:\ttest: 0.8337604\tbest: 0.8337604 (40)\ttotal: 7.78s\tremaining: 3m 1s\n",
      "41:\ttest: 0.8329057\tbest: 0.8337604 (40)\ttotal: 7.97s\tremaining: 3m 1s\n",
      "42:\ttest: 0.8336460\tbest: 0.8337604 (40)\ttotal: 8.17s\tremaining: 3m 1s\n",
      "43:\ttest: 0.8343384\tbest: 0.8343384 (43)\ttotal: 8.36s\tremaining: 3m 1s\n",
      "44:\ttest: 0.8346422\tbest: 0.8346422 (44)\ttotal: 8.56s\tremaining: 3m 1s\n",
      "45:\ttest: 0.8352626\tbest: 0.8352626 (45)\ttotal: 8.76s\tremaining: 3m 1s\n",
      "46:\ttest: 0.8361419\tbest: 0.8361419 (46)\ttotal: 8.96s\tremaining: 3m 1s\n",
      "47:\ttest: 0.8369267\tbest: 0.8369267 (47)\ttotal: 9.13s\tremaining: 3m 1s\n",
      "48:\ttest: 0.8373133\tbest: 0.8373133 (48)\ttotal: 9.34s\tremaining: 3m 1s\n",
      "49:\ttest: 0.8370146\tbest: 0.8373133 (48)\ttotal: 9.53s\tremaining: 3m 1s\n",
      "50:\ttest: 0.8367453\tbest: 0.8373133 (48)\ttotal: 9.74s\tremaining: 3m 1s\n",
      "51:\ttest: 0.8382306\tbest: 0.8382306 (51)\ttotal: 9.94s\tremaining: 3m 1s\n",
      "52:\ttest: 0.8387871\tbest: 0.8387871 (52)\ttotal: 10.1s\tremaining: 3m 1s\n",
      "53:\ttest: 0.8393774\tbest: 0.8393774 (53)\ttotal: 10.3s\tremaining: 3m 1s\n",
      "54:\ttest: 0.8399991\tbest: 0.8399991 (54)\ttotal: 10.5s\tremaining: 3m 1s\n",
      "55:\ttest: 0.8402735\tbest: 0.8402735 (55)\ttotal: 10.8s\tremaining: 3m 1s\n",
      "56:\ttest: 0.8409279\tbest: 0.8409279 (56)\ttotal: 11s\tremaining: 3m 1s\n",
      "57:\ttest: 0.8410924\tbest: 0.8410924 (57)\ttotal: 11.2s\tremaining: 3m 1s\n",
      "58:\ttest: 0.8417410\tbest: 0.8417410 (58)\ttotal: 11.4s\tremaining: 3m 1s\n",
      "59:\ttest: 0.8424099\tbest: 0.8424099 (59)\ttotal: 11.6s\tremaining: 3m 1s\n",
      "60:\ttest: 0.8425327\tbest: 0.8425327 (60)\ttotal: 11.7s\tremaining: 3m\n",
      "61:\ttest: 0.8431164\tbest: 0.8431164 (61)\ttotal: 11.9s\tremaining: 3m\n",
      "62:\ttest: 0.8428692\tbest: 0.8431164 (61)\ttotal: 12.1s\tremaining: 3m\n",
      "63:\ttest: 0.8430837\tbest: 0.8431164 (61)\ttotal: 12.3s\tremaining: 2m 59s\n",
      "64:\ttest: 0.8432392\tbest: 0.8432392 (64)\ttotal: 12.5s\tremaining: 3m\n",
      "65:\ttest: 0.8432811\tbest: 0.8432811 (65)\ttotal: 12.7s\tremaining: 3m\n",
      "66:\ttest: 0.8436089\tbest: 0.8436089 (66)\ttotal: 12.9s\tremaining: 2m 59s\n",
      "67:\ttest: 0.8439200\tbest: 0.8439200 (67)\ttotal: 13.1s\tremaining: 2m 59s\n",
      "68:\ttest: 0.8447243\tbest: 0.8447243 (68)\ttotal: 13.3s\tremaining: 2m 59s\n",
      "69:\ttest: 0.8450625\tbest: 0.8450625 (69)\ttotal: 13.5s\tremaining: 2m 59s\n",
      "70:\ttest: 0.8453234\tbest: 0.8453234 (70)\ttotal: 13.7s\tremaining: 2m 59s\n",
      "71:\ttest: 0.8458084\tbest: 0.8458084 (71)\ttotal: 13.9s\tremaining: 2m 59s\n",
      "72:\ttest: 0.8460161\tbest: 0.8460161 (72)\ttotal: 14.1s\tremaining: 2m 59s\n",
      "73:\ttest: 0.8458589\tbest: 0.8460161 (72)\ttotal: 14.3s\tremaining: 2m 59s\n",
      "74:\ttest: 0.8462118\tbest: 0.8462118 (74)\ttotal: 14.5s\tremaining: 2m 59s\n",
      "75:\ttest: 0.8464311\tbest: 0.8464311 (75)\ttotal: 14.7s\tremaining: 2m 58s\n",
      "76:\ttest: 0.8466087\tbest: 0.8466087 (76)\ttotal: 14.9s\tremaining: 2m 58s\n",
      "77:\ttest: 0.8471773\tbest: 0.8471773 (77)\ttotal: 15.1s\tremaining: 2m 58s\n",
      "78:\ttest: 0.8474628\tbest: 0.8474628 (78)\ttotal: 15.3s\tremaining: 2m 58s\n",
      "79:\ttest: 0.8475151\tbest: 0.8475151 (79)\ttotal: 15.5s\tremaining: 2m 58s\n",
      "80:\ttest: 0.8475681\tbest: 0.8475681 (80)\ttotal: 15.7s\tremaining: 2m 58s\n",
      "81:\ttest: 0.8476111\tbest: 0.8476111 (81)\ttotal: 15.9s\tremaining: 2m 57s\n",
      "82:\ttest: 0.8477584\tbest: 0.8477584 (82)\ttotal: 16.1s\tremaining: 2m 57s\n",
      "83:\ttest: 0.8477547\tbest: 0.8477584 (82)\ttotal: 16.3s\tremaining: 2m 57s\n",
      "84:\ttest: 0.8479666\tbest: 0.8479666 (84)\ttotal: 16.4s\tremaining: 2m 56s\n",
      "85:\ttest: 0.8481550\tbest: 0.8481550 (85)\ttotal: 16.6s\tremaining: 2m 56s\n",
      "86:\ttest: 0.8483068\tbest: 0.8483068 (86)\ttotal: 16.8s\tremaining: 2m 56s\n",
      "87:\ttest: 0.8485841\tbest: 0.8485841 (87)\ttotal: 17s\tremaining: 2m 56s\n",
      "88:\ttest: 0.8487100\tbest: 0.8487100 (88)\ttotal: 17.2s\tremaining: 2m 56s\n",
      "89:\ttest: 0.8491848\tbest: 0.8491848 (89)\ttotal: 17.4s\tremaining: 2m 55s\n",
      "90:\ttest: 0.8496683\tbest: 0.8496683 (90)\ttotal: 17.6s\tremaining: 2m 55s\n",
      "91:\ttest: 0.8499090\tbest: 0.8499090 (91)\ttotal: 17.8s\tremaining: 2m 55s\n",
      "92:\ttest: 0.8502976\tbest: 0.8502976 (92)\ttotal: 18s\tremaining: 2m 55s\n",
      "93:\ttest: 0.8504418\tbest: 0.8504418 (93)\ttotal: 18.2s\tremaining: 2m 55s\n",
      "94:\ttest: 0.8505332\tbest: 0.8505332 (94)\ttotal: 18.4s\tremaining: 2m 55s\n",
      "95:\ttest: 0.8508088\tbest: 0.8508088 (95)\ttotal: 18.6s\tremaining: 2m 54s\n",
      "96:\ttest: 0.8511462\tbest: 0.8511462 (96)\ttotal: 18.7s\tremaining: 2m 54s\n",
      "97:\ttest: 0.8514297\tbest: 0.8514297 (97)\ttotal: 18.9s\tremaining: 2m 54s\n",
      "98:\ttest: 0.8517036\tbest: 0.8517036 (98)\ttotal: 19.1s\tremaining: 2m 54s\n",
      "99:\ttest: 0.8518583\tbest: 0.8518583 (99)\ttotal: 19.3s\tremaining: 2m 53s\n",
      "100:\ttest: 0.8519134\tbest: 0.8519134 (100)\ttotal: 19.5s\tremaining: 2m 53s\n",
      "101:\ttest: 0.8520899\tbest: 0.8520899 (101)\ttotal: 19.7s\tremaining: 2m 53s\n",
      "102:\ttest: 0.8524082\tbest: 0.8524082 (102)\ttotal: 19.9s\tremaining: 2m 53s\n",
      "103:\ttest: 0.8526213\tbest: 0.8526213 (103)\ttotal: 20.1s\tremaining: 2m 53s\n",
      "104:\ttest: 0.8526808\tbest: 0.8526808 (104)\ttotal: 20.3s\tremaining: 2m 52s\n",
      "105:\ttest: 0.8527697\tbest: 0.8527697 (105)\ttotal: 20.5s\tremaining: 2m 52s\n",
      "106:\ttest: 0.8528594\tbest: 0.8528594 (106)\ttotal: 20.6s\tremaining: 2m 52s\n",
      "107:\ttest: 0.8529516\tbest: 0.8529516 (107)\ttotal: 20.8s\tremaining: 2m 52s\n",
      "108:\ttest: 0.8530634\tbest: 0.8530634 (108)\ttotal: 21s\tremaining: 2m 51s\n",
      "109:\ttest: 0.8534607\tbest: 0.8534607 (109)\ttotal: 21.2s\tremaining: 2m 51s\n",
      "110:\ttest: 0.8533857\tbest: 0.8534607 (109)\ttotal: 21.4s\tremaining: 2m 51s\n",
      "111:\ttest: 0.8536868\tbest: 0.8536868 (111)\ttotal: 21.6s\tremaining: 2m 51s\n",
      "112:\ttest: 0.8539992\tbest: 0.8539992 (112)\ttotal: 21.8s\tremaining: 2m 51s\n",
      "113:\ttest: 0.8540474\tbest: 0.8540474 (113)\ttotal: 22s\tremaining: 2m 50s\n",
      "114:\ttest: 0.8542725\tbest: 0.8542725 (114)\ttotal: 22.2s\tremaining: 2m 50s\n",
      "115:\ttest: 0.8544113\tbest: 0.8544113 (115)\ttotal: 22.4s\tremaining: 2m 50s\n",
      "116:\ttest: 0.8546003\tbest: 0.8546003 (116)\ttotal: 22.6s\tremaining: 2m 50s\n",
      "117:\ttest: 0.8546824\tbest: 0.8546824 (117)\ttotal: 22.8s\tremaining: 2m 50s\n",
      "118:\ttest: 0.8549278\tbest: 0.8549278 (118)\ttotal: 23s\tremaining: 2m 50s\n",
      "119:\ttest: 0.8549648\tbest: 0.8549648 (119)\ttotal: 23.2s\tremaining: 2m 49s\n",
      "120:\ttest: 0.8549429\tbest: 0.8549648 (119)\ttotal: 23.3s\tremaining: 2m 49s\n",
      "121:\ttest: 0.8549776\tbest: 0.8549776 (121)\ttotal: 23.5s\tremaining: 2m 49s\n",
      "122:\ttest: 0.8549562\tbest: 0.8549776 (121)\ttotal: 23.7s\tremaining: 2m 49s\n",
      "123:\ttest: 0.8551870\tbest: 0.8551870 (123)\ttotal: 23.9s\tremaining: 2m 49s\n",
      "124:\ttest: 0.8554886\tbest: 0.8554886 (124)\ttotal: 24.1s\tremaining: 2m 48s\n",
      "125:\ttest: 0.8555460\tbest: 0.8555460 (125)\ttotal: 24.3s\tremaining: 2m 48s\n",
      "126:\ttest: 0.8558879\tbest: 0.8558879 (126)\ttotal: 24.5s\tremaining: 2m 48s\n",
      "127:\ttest: 0.8559030\tbest: 0.8559030 (127)\ttotal: 24.8s\tremaining: 2m 48s\n",
      "128:\ttest: 0.8563031\tbest: 0.8563031 (128)\ttotal: 25s\tremaining: 2m 48s\n",
      "129:\ttest: 0.8563037\tbest: 0.8563037 (129)\ttotal: 25.2s\tremaining: 2m 48s\n",
      "130:\ttest: 0.8563868\tbest: 0.8563868 (130)\ttotal: 25.3s\tremaining: 2m 48s\n",
      "131:\ttest: 0.8565254\tbest: 0.8565254 (131)\ttotal: 25.5s\tremaining: 2m 47s\n",
      "132:\ttest: 0.8566433\tbest: 0.8566433 (132)\ttotal: 25.7s\tremaining: 2m 47s\n",
      "133:\ttest: 0.8570005\tbest: 0.8570005 (133)\ttotal: 25.9s\tremaining: 2m 47s\n",
      "134:\ttest: 0.8570579\tbest: 0.8570579 (134)\ttotal: 26.1s\tremaining: 2m 46s\n",
      "135:\ttest: 0.8571913\tbest: 0.8571913 (135)\ttotal: 26.2s\tremaining: 2m 46s\n",
      "136:\ttest: 0.8572335\tbest: 0.8572335 (136)\ttotal: 26.4s\tremaining: 2m 46s\n",
      "137:\ttest: 0.8572590\tbest: 0.8572590 (137)\ttotal: 26.6s\tremaining: 2m 46s\n",
      "138:\ttest: 0.8573152\tbest: 0.8573152 (138)\ttotal: 26.8s\tremaining: 2m 45s\n",
      "139:\ttest: 0.8574338\tbest: 0.8574338 (139)\ttotal: 27s\tremaining: 2m 45s\n",
      "140:\ttest: 0.8577604\tbest: 0.8577604 (140)\ttotal: 27.2s\tremaining: 2m 45s\n",
      "141:\ttest: 0.8577853\tbest: 0.8577853 (141)\ttotal: 27.3s\tremaining: 2m 45s\n",
      "142:\ttest: 0.8578939\tbest: 0.8578939 (142)\ttotal: 27.5s\tremaining: 2m 45s\n",
      "143:\ttest: 0.8580278\tbest: 0.8580278 (143)\ttotal: 27.7s\tremaining: 2m 44s\n",
      "144:\ttest: 0.8580663\tbest: 0.8580663 (144)\ttotal: 27.9s\tremaining: 2m 44s\n",
      "145:\ttest: 0.8582305\tbest: 0.8582305 (145)\ttotal: 28.1s\tremaining: 2m 44s\n",
      "146:\ttest: 0.8582853\tbest: 0.8582853 (146)\ttotal: 28.3s\tremaining: 2m 44s\n",
      "147:\ttest: 0.8583147\tbest: 0.8583147 (147)\ttotal: 28.5s\tremaining: 2m 44s\n",
      "148:\ttest: 0.8586480\tbest: 0.8586480 (148)\ttotal: 28.7s\tremaining: 2m 44s\n",
      "149:\ttest: 0.8587137\tbest: 0.8587137 (149)\ttotal: 28.9s\tremaining: 2m 43s\n",
      "150:\ttest: 0.8587358\tbest: 0.8587358 (150)\ttotal: 29.1s\tremaining: 2m 43s\n",
      "151:\ttest: 0.8588659\tbest: 0.8588659 (151)\ttotal: 29.3s\tremaining: 2m 43s\n",
      "152:\ttest: 0.8589370\tbest: 0.8589370 (152)\ttotal: 29.5s\tremaining: 2m 43s\n",
      "153:\ttest: 0.8589832\tbest: 0.8589832 (153)\ttotal: 29.7s\tremaining: 2m 42s\n",
      "154:\ttest: 0.8590414\tbest: 0.8590414 (154)\ttotal: 29.9s\tremaining: 2m 42s\n",
      "155:\ttest: 0.8590339\tbest: 0.8590414 (154)\ttotal: 30.1s\tremaining: 2m 42s\n",
      "156:\ttest: 0.8590102\tbest: 0.8590414 (154)\ttotal: 30.3s\tremaining: 2m 42s\n",
      "157:\ttest: 0.8590230\tbest: 0.8590414 (154)\ttotal: 30.4s\tremaining: 2m 42s\n",
      "158:\ttest: 0.8591423\tbest: 0.8591423 (158)\ttotal: 30.6s\tremaining: 2m 41s\n",
      "159:\ttest: 0.8591315\tbest: 0.8591423 (158)\ttotal: 30.8s\tremaining: 2m 41s\n",
      "160:\ttest: 0.8592455\tbest: 0.8592455 (160)\ttotal: 30.9s\tremaining: 2m 41s\n",
      "161:\ttest: 0.8593839\tbest: 0.8593839 (161)\ttotal: 31.1s\tremaining: 2m 41s\n",
      "162:\ttest: 0.8595406\tbest: 0.8595406 (162)\ttotal: 31.3s\tremaining: 2m 40s\n",
      "163:\ttest: 0.8596417\tbest: 0.8596417 (163)\ttotal: 31.5s\tremaining: 2m 40s\n",
      "164:\ttest: 0.8598379\tbest: 0.8598379 (164)\ttotal: 31.7s\tremaining: 2m 40s\n",
      "165:\ttest: 0.8598843\tbest: 0.8598843 (165)\ttotal: 31.9s\tremaining: 2m 40s\n",
      "166:\ttest: 0.8601146\tbest: 0.8601146 (166)\ttotal: 32.1s\tremaining: 2m 40s\n",
      "167:\ttest: 0.8601546\tbest: 0.8601546 (167)\ttotal: 32.3s\tremaining: 2m 39s\n",
      "168:\ttest: 0.8602486\tbest: 0.8602486 (168)\ttotal: 32.5s\tremaining: 2m 39s\n",
      "169:\ttest: 0.8603132\tbest: 0.8603132 (169)\ttotal: 32.7s\tremaining: 2m 39s\n",
      "170:\ttest: 0.8604098\tbest: 0.8604098 (170)\ttotal: 32.8s\tremaining: 2m 39s\n",
      "171:\ttest: 0.8603564\tbest: 0.8604098 (170)\ttotal: 33s\tremaining: 2m 38s\n",
      "172:\ttest: 0.8603380\tbest: 0.8604098 (170)\ttotal: 33.2s\tremaining: 2m 38s\n",
      "173:\ttest: 0.8603747\tbest: 0.8604098 (170)\ttotal: 33.3s\tremaining: 2m 38s\n",
      "174:\ttest: 0.8603507\tbest: 0.8604098 (170)\ttotal: 33.5s\tremaining: 2m 37s\n",
      "175:\ttest: 0.8604561\tbest: 0.8604561 (175)\ttotal: 33.7s\tremaining: 2m 37s\n",
      "176:\ttest: 0.8606310\tbest: 0.8606310 (176)\ttotal: 33.9s\tremaining: 2m 37s\n",
      "177:\ttest: 0.8606434\tbest: 0.8606434 (177)\ttotal: 34.1s\tremaining: 2m 37s\n",
      "178:\ttest: 0.8608474\tbest: 0.8608474 (178)\ttotal: 34.3s\tremaining: 2m 37s\n",
      "179:\ttest: 0.8609261\tbest: 0.8609261 (179)\ttotal: 34.5s\tremaining: 2m 37s\n",
      "180:\ttest: 0.8609204\tbest: 0.8609261 (179)\ttotal: 34.7s\tremaining: 2m 36s\n",
      "181:\ttest: 0.8609274\tbest: 0.8609274 (181)\ttotal: 34.9s\tremaining: 2m 36s\n",
      "182:\ttest: 0.8611408\tbest: 0.8611408 (182)\ttotal: 35.1s\tremaining: 2m 36s\n",
      "183:\ttest: 0.8611728\tbest: 0.8611728 (183)\ttotal: 35.3s\tremaining: 2m 36s\n",
      "184:\ttest: 0.8612202\tbest: 0.8612202 (184)\ttotal: 35.5s\tremaining: 2m 36s\n",
      "185:\ttest: 0.8612659\tbest: 0.8612659 (185)\ttotal: 35.7s\tremaining: 2m 36s\n",
      "186:\ttest: 0.8613391\tbest: 0.8613391 (186)\ttotal: 35.9s\tremaining: 2m 35s\n",
      "187:\ttest: 0.8614958\tbest: 0.8614958 (187)\ttotal: 36.1s\tremaining: 2m 35s\n",
      "188:\ttest: 0.8615751\tbest: 0.8615751 (188)\ttotal: 36.3s\tremaining: 2m 35s\n",
      "189:\ttest: 0.8616494\tbest: 0.8616494 (189)\ttotal: 36.5s\tremaining: 2m 35s\n",
      "190:\ttest: 0.8616906\tbest: 0.8616906 (190)\ttotal: 36.7s\tremaining: 2m 35s\n",
      "191:\ttest: 0.8617856\tbest: 0.8617856 (191)\ttotal: 36.9s\tremaining: 2m 35s\n",
      "192:\ttest: 0.8618323\tbest: 0.8618323 (192)\ttotal: 37s\tremaining: 2m 34s\n",
      "193:\ttest: 0.8618350\tbest: 0.8618350 (193)\ttotal: 37.2s\tremaining: 2m 34s\n",
      "194:\ttest: 0.8618898\tbest: 0.8618898 (194)\ttotal: 37.4s\tremaining: 2m 34s\n",
      "195:\ttest: 0.8619344\tbest: 0.8619344 (195)\ttotal: 37.6s\tremaining: 2m 34s\n",
      "196:\ttest: 0.8620673\tbest: 0.8620673 (196)\ttotal: 37.8s\tremaining: 2m 33s\n",
      "197:\ttest: 0.8620865\tbest: 0.8620865 (197)\ttotal: 38s\tremaining: 2m 33s\n",
      "198:\ttest: 0.8620654\tbest: 0.8620865 (197)\ttotal: 38.1s\tremaining: 2m 33s\n",
      "199:\ttest: 0.8621289\tbest: 0.8621289 (199)\ttotal: 38.3s\tremaining: 2m 33s\n",
      "200:\ttest: 0.8621425\tbest: 0.8621425 (200)\ttotal: 38.5s\tremaining: 2m 32s\n",
      "201:\ttest: 0.8621882\tbest: 0.8621882 (201)\ttotal: 38.7s\tremaining: 2m 32s\n",
      "202:\ttest: 0.8622119\tbest: 0.8622119 (202)\ttotal: 38.8s\tremaining: 2m 32s\n",
      "203:\ttest: 0.8622628\tbest: 0.8622628 (203)\ttotal: 39s\tremaining: 2m 32s\n",
      "204:\ttest: 0.8623119\tbest: 0.8623119 (204)\ttotal: 39.2s\tremaining: 2m 31s\n",
      "205:\ttest: 0.8623849\tbest: 0.8623849 (205)\ttotal: 39.3s\tremaining: 2m 31s\n",
      "206:\ttest: 0.8624108\tbest: 0.8624108 (206)\ttotal: 39.5s\tremaining: 2m 31s\n",
      "207:\ttest: 0.8625187\tbest: 0.8625187 (207)\ttotal: 39.7s\tremaining: 2m 31s\n",
      "208:\ttest: 0.8626335\tbest: 0.8626335 (208)\ttotal: 39.9s\tremaining: 2m 30s\n",
      "209:\ttest: 0.8627221\tbest: 0.8627221 (209)\ttotal: 40.1s\tremaining: 2m 30s\n",
      "210:\ttest: 0.8627662\tbest: 0.8627662 (210)\ttotal: 40.3s\tremaining: 2m 30s\n",
      "211:\ttest: 0.8628100\tbest: 0.8628100 (211)\ttotal: 40.5s\tremaining: 2m 30s\n",
      "212:\ttest: 0.8628766\tbest: 0.8628766 (212)\ttotal: 40.6s\tremaining: 2m 30s\n",
      "213:\ttest: 0.8628756\tbest: 0.8628766 (212)\ttotal: 40.8s\tremaining: 2m 29s\n",
      "214:\ttest: 0.8629185\tbest: 0.8629185 (214)\ttotal: 41s\tremaining: 2m 29s\n",
      "215:\ttest: 0.8629570\tbest: 0.8629570 (215)\ttotal: 41.2s\tremaining: 2m 29s\n",
      "216:\ttest: 0.8629502\tbest: 0.8629570 (215)\ttotal: 41.4s\tremaining: 2m 29s\n",
      "217:\ttest: 0.8629537\tbest: 0.8629570 (215)\ttotal: 41.6s\tremaining: 2m 29s\n",
      "218:\ttest: 0.8629990\tbest: 0.8629990 (218)\ttotal: 41.8s\tremaining: 2m 29s\n",
      "219:\ttest: 0.8631503\tbest: 0.8631503 (219)\ttotal: 42s\tremaining: 2m 28s\n",
      "220:\ttest: 0.8631953\tbest: 0.8631953 (220)\ttotal: 42.2s\tremaining: 2m 28s\n",
      "221:\ttest: 0.8633133\tbest: 0.8633133 (221)\ttotal: 42.4s\tremaining: 2m 28s\n",
      "222:\ttest: 0.8633358\tbest: 0.8633358 (222)\ttotal: 42.5s\tremaining: 2m 28s\n",
      "223:\ttest: 0.8634012\tbest: 0.8634012 (223)\ttotal: 42.8s\tremaining: 2m 28s\n",
      "224:\ttest: 0.8634452\tbest: 0.8634452 (224)\ttotal: 42.9s\tremaining: 2m 27s\n",
      "225:\ttest: 0.8634505\tbest: 0.8634505 (225)\ttotal: 43.1s\tremaining: 2m 27s\n",
      "226:\ttest: 0.8635660\tbest: 0.8635660 (226)\ttotal: 43.3s\tremaining: 2m 27s\n",
      "227:\ttest: 0.8636207\tbest: 0.8636207 (227)\ttotal: 43.5s\tremaining: 2m 27s\n",
      "228:\ttest: 0.8637659\tbest: 0.8637659 (228)\ttotal: 43.6s\tremaining: 2m 26s\n",
      "229:\ttest: 0.8638518\tbest: 0.8638518 (229)\ttotal: 43.9s\tremaining: 2m 26s\n",
      "230:\ttest: 0.8638424\tbest: 0.8638518 (229)\ttotal: 44s\tremaining: 2m 26s\n",
      "231:\ttest: 0.8639501\tbest: 0.8639501 (231)\ttotal: 44.2s\tremaining: 2m 26s\n",
      "232:\ttest: 0.8640682\tbest: 0.8640682 (232)\ttotal: 44.4s\tremaining: 2m 26s\n",
      "233:\ttest: 0.8642056\tbest: 0.8642056 (233)\ttotal: 44.5s\tremaining: 2m 25s\n",
      "234:\ttest: 0.8641996\tbest: 0.8642056 (233)\ttotal: 44.7s\tremaining: 2m 25s\n",
      "235:\ttest: 0.8642267\tbest: 0.8642267 (235)\ttotal: 44.9s\tremaining: 2m 25s\n",
      "236:\ttest: 0.8642437\tbest: 0.8642437 (236)\ttotal: 45.1s\tremaining: 2m 25s\n",
      "237:\ttest: 0.8643183\tbest: 0.8643183 (237)\ttotal: 45.3s\tremaining: 2m 25s\n",
      "238:\ttest: 0.8644240\tbest: 0.8644240 (238)\ttotal: 45.5s\tremaining: 2m 24s\n",
      "239:\ttest: 0.8644841\tbest: 0.8644841 (239)\ttotal: 45.6s\tremaining: 2m 24s\n",
      "240:\ttest: 0.8645534\tbest: 0.8645534 (240)\ttotal: 45.8s\tremaining: 2m 24s\n",
      "241:\ttest: 0.8645473\tbest: 0.8645534 (240)\ttotal: 46s\tremaining: 2m 24s\n",
      "242:\ttest: 0.8645494\tbest: 0.8645534 (240)\ttotal: 46.2s\tremaining: 2m 23s\n",
      "243:\ttest: 0.8646435\tbest: 0.8646435 (243)\ttotal: 46.4s\tremaining: 2m 23s\n",
      "244:\ttest: 0.8647191\tbest: 0.8647191 (244)\ttotal: 46.6s\tremaining: 2m 23s\n",
      "245:\ttest: 0.8647125\tbest: 0.8647191 (244)\ttotal: 46.8s\tremaining: 2m 23s\n",
      "246:\ttest: 0.8648010\tbest: 0.8648010 (246)\ttotal: 47s\tremaining: 2m 23s\n",
      "247:\ttest: 0.8648459\tbest: 0.8648459 (247)\ttotal: 47.2s\tremaining: 2m 23s\n",
      "248:\ttest: 0.8649381\tbest: 0.8649381 (248)\ttotal: 47.4s\tremaining: 2m 22s\n",
      "249:\ttest: 0.8649717\tbest: 0.8649717 (249)\ttotal: 47.6s\tremaining: 2m 22s\n",
      "250:\ttest: 0.8650288\tbest: 0.8650288 (250)\ttotal: 47.7s\tremaining: 2m 22s\n",
      "251:\ttest: 0.8650399\tbest: 0.8650399 (251)\ttotal: 47.9s\tremaining: 2m 22s\n",
      "252:\ttest: 0.8650511\tbest: 0.8650511 (252)\ttotal: 48.1s\tremaining: 2m 22s\n",
      "253:\ttest: 0.8650456\tbest: 0.8650511 (252)\ttotal: 48.3s\tremaining: 2m 21s\n",
      "254:\ttest: 0.8651173\tbest: 0.8651173 (254)\ttotal: 48.4s\tremaining: 2m 21s\n",
      "255:\ttest: 0.8651683\tbest: 0.8651683 (255)\ttotal: 48.6s\tremaining: 2m 21s\n",
      "256:\ttest: 0.8651801\tbest: 0.8651801 (256)\ttotal: 48.8s\tremaining: 2m 21s\n",
      "257:\ttest: 0.8651949\tbest: 0.8651949 (257)\ttotal: 49s\tremaining: 2m 20s\n",
      "258:\ttest: 0.8652535\tbest: 0.8652535 (258)\ttotal: 49.2s\tremaining: 2m 20s\n",
      "259:\ttest: 0.8652841\tbest: 0.8652841 (259)\ttotal: 49.4s\tremaining: 2m 20s\n",
      "260:\ttest: 0.8653177\tbest: 0.8653177 (260)\ttotal: 49.5s\tremaining: 2m 20s\n",
      "261:\ttest: 0.8653847\tbest: 0.8653847 (261)\ttotal: 49.7s\tremaining: 2m 20s\n",
      "262:\ttest: 0.8654074\tbest: 0.8654074 (262)\ttotal: 49.9s\tremaining: 2m 19s\n",
      "263:\ttest: 0.8654744\tbest: 0.8654744 (263)\ttotal: 50.1s\tremaining: 2m 19s\n",
      "264:\ttest: 0.8656225\tbest: 0.8656225 (264)\ttotal: 50.3s\tremaining: 2m 19s\n",
      "265:\ttest: 0.8656363\tbest: 0.8656363 (265)\ttotal: 50.4s\tremaining: 2m 19s\n",
      "266:\ttest: 0.8656682\tbest: 0.8656682 (266)\ttotal: 50.6s\tremaining: 2m 19s\n",
      "267:\ttest: 0.8656593\tbest: 0.8656682 (266)\ttotal: 50.8s\tremaining: 2m 18s\n",
      "268:\ttest: 0.8656544\tbest: 0.8656682 (266)\ttotal: 51s\tremaining: 2m 18s\n",
      "269:\ttest: 0.8656588\tbest: 0.8656682 (266)\ttotal: 51.2s\tremaining: 2m 18s\n",
      "270:\ttest: 0.8656906\tbest: 0.8656906 (270)\ttotal: 51.4s\tremaining: 2m 18s\n",
      "271:\ttest: 0.8657002\tbest: 0.8657002 (271)\ttotal: 51.6s\tremaining: 2m 18s\n",
      "272:\ttest: 0.8657491\tbest: 0.8657491 (272)\ttotal: 51.8s\tremaining: 2m 17s\n",
      "273:\ttest: 0.8657802\tbest: 0.8657802 (273)\ttotal: 52s\tremaining: 2m 17s\n",
      "274:\ttest: 0.8657893\tbest: 0.8657893 (274)\ttotal: 52.2s\tremaining: 2m 17s\n",
      "275:\ttest: 0.8657433\tbest: 0.8657893 (274)\ttotal: 52.3s\tremaining: 2m 17s\n",
      "276:\ttest: 0.8657995\tbest: 0.8657995 (276)\ttotal: 52.5s\tremaining: 2m 17s\n",
      "277:\ttest: 0.8658637\tbest: 0.8658637 (277)\ttotal: 52.7s\tremaining: 2m 16s\n",
      "278:\ttest: 0.8659644\tbest: 0.8659644 (278)\ttotal: 52.9s\tremaining: 2m 16s\n",
      "279:\ttest: 0.8659661\tbest: 0.8659661 (279)\ttotal: 53.1s\tremaining: 2m 16s\n",
      "280:\ttest: 0.8659671\tbest: 0.8659671 (280)\ttotal: 53.3s\tremaining: 2m 16s\n",
      "281:\ttest: 0.8659934\tbest: 0.8659934 (281)\ttotal: 53.4s\tremaining: 2m 16s\n",
      "282:\ttest: 0.8660592\tbest: 0.8660592 (282)\ttotal: 53.6s\tremaining: 2m 15s\n",
      "283:\ttest: 0.8660742\tbest: 0.8660742 (283)\ttotal: 53.8s\tremaining: 2m 15s\n",
      "284:\ttest: 0.8661328\tbest: 0.8661328 (284)\ttotal: 54s\tremaining: 2m 15s\n",
      "285:\ttest: 0.8661490\tbest: 0.8661490 (285)\ttotal: 54.2s\tremaining: 2m 15s\n",
      "286:\ttest: 0.8661761\tbest: 0.8661761 (286)\ttotal: 54.4s\tremaining: 2m 15s\n",
      "287:\ttest: 0.8661874\tbest: 0.8661874 (287)\ttotal: 54.6s\tremaining: 2m 14s\n",
      "288:\ttest: 0.8661642\tbest: 0.8661874 (287)\ttotal: 54.8s\tremaining: 2m 14s\n",
      "289:\ttest: 0.8661758\tbest: 0.8661874 (287)\ttotal: 54.9s\tremaining: 2m 14s\n",
      "290:\ttest: 0.8662036\tbest: 0.8662036 (290)\ttotal: 55.1s\tremaining: 2m 14s\n",
      "291:\ttest: 0.8662238\tbest: 0.8662238 (291)\ttotal: 55.3s\tremaining: 2m 14s\n",
      "292:\ttest: 0.8662946\tbest: 0.8662946 (292)\ttotal: 55.5s\tremaining: 2m 13s\n",
      "293:\ttest: 0.8662871\tbest: 0.8662946 (292)\ttotal: 55.7s\tremaining: 2m 13s\n",
      "294:\ttest: 0.8662744\tbest: 0.8662946 (292)\ttotal: 55.9s\tremaining: 2m 13s\n",
      "295:\ttest: 0.8663119\tbest: 0.8663119 (295)\ttotal: 56s\tremaining: 2m 13s\n",
      "296:\ttest: 0.8663102\tbest: 0.8663119 (295)\ttotal: 56.2s\tremaining: 2m 13s\n",
      "297:\ttest: 0.8663714\tbest: 0.8663714 (297)\ttotal: 56.4s\tremaining: 2m 12s\n",
      "298:\ttest: 0.8664238\tbest: 0.8664238 (298)\ttotal: 56.6s\tremaining: 2m 12s\n",
      "299:\ttest: 0.8664585\tbest: 0.8664585 (299)\ttotal: 56.8s\tremaining: 2m 12s\n",
      "300:\ttest: 0.8664989\tbest: 0.8664989 (300)\ttotal: 57s\tremaining: 2m 12s\n",
      "301:\ttest: 0.8666034\tbest: 0.8666034 (301)\ttotal: 57.2s\tremaining: 2m 12s\n",
      "302:\ttest: 0.8666760\tbest: 0.8666760 (302)\ttotal: 57.4s\tremaining: 2m 11s\n",
      "303:\ttest: 0.8666778\tbest: 0.8666778 (303)\ttotal: 57.5s\tremaining: 2m 11s\n",
      "304:\ttest: 0.8667575\tbest: 0.8667575 (304)\ttotal: 57.8s\tremaining: 2m 11s\n",
      "305:\ttest: 0.8667719\tbest: 0.8667719 (305)\ttotal: 57.9s\tremaining: 2m 11s\n",
      "306:\ttest: 0.8668341\tbest: 0.8668341 (306)\ttotal: 58.1s\tremaining: 2m 11s\n",
      "307:\ttest: 0.8668643\tbest: 0.8668643 (307)\ttotal: 58.3s\tremaining: 2m 11s\n",
      "308:\ttest: 0.8668842\tbest: 0.8668842 (308)\ttotal: 58.5s\tremaining: 2m 10s\n",
      "309:\ttest: 0.8668886\tbest: 0.8668886 (309)\ttotal: 58.7s\tremaining: 2m 10s\n",
      "310:\ttest: 0.8669331\tbest: 0.8669331 (310)\ttotal: 58.9s\tremaining: 2m 10s\n",
      "311:\ttest: 0.8669905\tbest: 0.8669905 (311)\ttotal: 59.1s\tremaining: 2m 10s\n",
      "312:\ttest: 0.8669948\tbest: 0.8669948 (312)\ttotal: 59.2s\tremaining: 2m 10s\n",
      "313:\ttest: 0.8669894\tbest: 0.8669948 (312)\ttotal: 59.4s\tremaining: 2m 9s\n",
      "314:\ttest: 0.8669964\tbest: 0.8669964 (314)\ttotal: 59.6s\tremaining: 2m 9s\n",
      "315:\ttest: 0.8670442\tbest: 0.8670442 (315)\ttotal: 59.8s\tremaining: 2m 9s\n",
      "316:\ttest: 0.8670361\tbest: 0.8670442 (315)\ttotal: 60s\tremaining: 2m 9s\n",
      "317:\ttest: 0.8670345\tbest: 0.8670442 (315)\ttotal: 1m\tremaining: 2m 9s\n",
      "318:\ttest: 0.8671079\tbest: 0.8671079 (318)\ttotal: 1m\tremaining: 2m 8s\n",
      "319:\ttest: 0.8671046\tbest: 0.8671079 (318)\ttotal: 1m\tremaining: 2m 8s\n",
      "320:\ttest: 0.8671444\tbest: 0.8671444 (320)\ttotal: 1m\tremaining: 2m 8s\n",
      "321:\ttest: 0.8671699\tbest: 0.8671699 (321)\ttotal: 1m\tremaining: 2m 8s\n",
      "322:\ttest: 0.8672074\tbest: 0.8672074 (322)\ttotal: 1m 1s\tremaining: 2m 8s\n",
      "323:\ttest: 0.8672597\tbest: 0.8672597 (323)\ttotal: 1m 1s\tremaining: 2m 7s\n",
      "324:\ttest: 0.8672997\tbest: 0.8672997 (324)\ttotal: 1m 1s\tremaining: 2m 7s\n",
      "325:\ttest: 0.8673182\tbest: 0.8673182 (325)\ttotal: 1m 1s\tremaining: 2m 7s\n",
      "326:\ttest: 0.8673538\tbest: 0.8673538 (326)\ttotal: 1m 1s\tremaining: 2m 7s\n",
      "327:\ttest: 0.8673880\tbest: 0.8673880 (327)\ttotal: 1m 2s\tremaining: 2m 7s\n",
      "328:\ttest: 0.8673789\tbest: 0.8673880 (327)\ttotal: 1m 2s\tremaining: 2m 6s\n",
      "329:\ttest: 0.8674023\tbest: 0.8674023 (329)\ttotal: 1m 2s\tremaining: 2m 6s\n",
      "330:\ttest: 0.8673986\tbest: 0.8674023 (329)\ttotal: 1m 2s\tremaining: 2m 6s\n",
      "331:\ttest: 0.8674263\tbest: 0.8674263 (331)\ttotal: 1m 2s\tremaining: 2m 6s\n",
      "332:\ttest: 0.8674829\tbest: 0.8674829 (332)\ttotal: 1m 2s\tremaining: 2m 6s\n",
      "333:\ttest: 0.8674856\tbest: 0.8674856 (333)\ttotal: 1m 3s\tremaining: 2m 5s\n",
      "334:\ttest: 0.8675147\tbest: 0.8675147 (334)\ttotal: 1m 3s\tremaining: 2m 5s\n",
      "335:\ttest: 0.8675401\tbest: 0.8675401 (335)\ttotal: 1m 3s\tremaining: 2m 5s\n",
      "336:\ttest: 0.8675533\tbest: 0.8675533 (336)\ttotal: 1m 3s\tremaining: 2m 5s\n",
      "337:\ttest: 0.8676135\tbest: 0.8676135 (337)\ttotal: 1m 3s\tremaining: 2m 5s\n",
      "338:\ttest: 0.8676125\tbest: 0.8676135 (337)\ttotal: 1m 4s\tremaining: 2m 4s\n",
      "339:\ttest: 0.8676423\tbest: 0.8676423 (339)\ttotal: 1m 4s\tremaining: 2m 4s\n",
      "340:\ttest: 0.8676836\tbest: 0.8676836 (340)\ttotal: 1m 4s\tremaining: 2m 4s\n",
      "341:\ttest: 0.8676594\tbest: 0.8676836 (340)\ttotal: 1m 4s\tremaining: 2m 4s\n",
      "342:\ttest: 0.8677222\tbest: 0.8677222 (342)\ttotal: 1m 4s\tremaining: 2m 4s\n",
      "343:\ttest: 0.8678157\tbest: 0.8678157 (343)\ttotal: 1m 4s\tremaining: 2m 3s\n",
      "344:\ttest: 0.8678660\tbest: 0.8678660 (344)\ttotal: 1m 5s\tremaining: 2m 3s\n",
      "345:\ttest: 0.8678562\tbest: 0.8678660 (344)\ttotal: 1m 5s\tremaining: 2m 3s\n",
      "346:\ttest: 0.8678646\tbest: 0.8678660 (344)\ttotal: 1m 5s\tremaining: 2m 3s\n",
      "347:\ttest: 0.8679594\tbest: 0.8679594 (347)\ttotal: 1m 5s\tremaining: 2m 3s\n",
      "348:\ttest: 0.8679807\tbest: 0.8679807 (348)\ttotal: 1m 5s\tremaining: 2m 2s\n",
      "349:\ttest: 0.8680029\tbest: 0.8680029 (349)\ttotal: 1m 6s\tremaining: 2m 2s\n",
      "350:\ttest: 0.8680402\tbest: 0.8680402 (350)\ttotal: 1m 6s\tremaining: 2m 2s\n",
      "351:\ttest: 0.8680477\tbest: 0.8680477 (351)\ttotal: 1m 6s\tremaining: 2m 2s\n",
      "352:\ttest: 0.8680594\tbest: 0.8680594 (352)\ttotal: 1m 6s\tremaining: 2m 2s\n",
      "353:\ttest: 0.8680638\tbest: 0.8680638 (353)\ttotal: 1m 6s\tremaining: 2m 1s\n",
      "354:\ttest: 0.8680467\tbest: 0.8680638 (353)\ttotal: 1m 7s\tremaining: 2m 1s\n",
      "355:\ttest: 0.8680692\tbest: 0.8680692 (355)\ttotal: 1m 7s\tremaining: 2m 1s\n",
      "356:\ttest: 0.8680886\tbest: 0.8680886 (356)\ttotal: 1m 7s\tremaining: 2m 1s\n",
      "357:\ttest: 0.8681531\tbest: 0.8681531 (357)\ttotal: 1m 7s\tremaining: 2m 1s\n",
      "358:\ttest: 0.8681648\tbest: 0.8681648 (358)\ttotal: 1m 7s\tremaining: 2m\n",
      "359:\ttest: 0.8681921\tbest: 0.8681921 (359)\ttotal: 1m 7s\tremaining: 2m\n",
      "360:\ttest: 0.8682639\tbest: 0.8682639 (360)\ttotal: 1m 8s\tremaining: 2m\n",
      "361:\ttest: 0.8683007\tbest: 0.8683007 (361)\ttotal: 1m 8s\tremaining: 2m\n",
      "362:\ttest: 0.8683311\tbest: 0.8683311 (362)\ttotal: 1m 8s\tremaining: 2m\n",
      "363:\ttest: 0.8683414\tbest: 0.8683414 (363)\ttotal: 1m 8s\tremaining: 1m 59s\n",
      "364:\ttest: 0.8683784\tbest: 0.8683784 (364)\ttotal: 1m 8s\tremaining: 1m 59s\n",
      "365:\ttest: 0.8683858\tbest: 0.8683858 (365)\ttotal: 1m 8s\tremaining: 1m 59s\n",
      "366:\ttest: 0.8684083\tbest: 0.8684083 (366)\ttotal: 1m 9s\tremaining: 1m 59s\n",
      "367:\ttest: 0.8684261\tbest: 0.8684261 (367)\ttotal: 1m 9s\tremaining: 1m 59s\n",
      "368:\ttest: 0.8684564\tbest: 0.8684564 (368)\ttotal: 1m 9s\tremaining: 1m 58s\n",
      "369:\ttest: 0.8684613\tbest: 0.8684613 (369)\ttotal: 1m 9s\tremaining: 1m 58s\n",
      "370:\ttest: 0.8684814\tbest: 0.8684814 (370)\ttotal: 1m 9s\tremaining: 1m 58s\n",
      "371:\ttest: 0.8684983\tbest: 0.8684983 (371)\ttotal: 1m 10s\tremaining: 1m 58s\n",
      "372:\ttest: 0.8684992\tbest: 0.8684992 (372)\ttotal: 1m 10s\tremaining: 1m 58s\n",
      "373:\ttest: 0.8685490\tbest: 0.8685490 (373)\ttotal: 1m 10s\tremaining: 1m 57s\n",
      "374:\ttest: 0.8685582\tbest: 0.8685582 (374)\ttotal: 1m 10s\tremaining: 1m 57s\n",
      "375:\ttest: 0.8686130\tbest: 0.8686130 (375)\ttotal: 1m 10s\tremaining: 1m 57s\n",
      "376:\ttest: 0.8686402\tbest: 0.8686402 (376)\ttotal: 1m 10s\tremaining: 1m 57s\n",
      "377:\ttest: 0.8687371\tbest: 0.8687371 (377)\ttotal: 1m 11s\tremaining: 1m 56s\n",
      "378:\ttest: 0.8687415\tbest: 0.8687415 (378)\ttotal: 1m 11s\tremaining: 1m 56s\n",
      "379:\ttest: 0.8688001\tbest: 0.8688001 (379)\ttotal: 1m 11s\tremaining: 1m 56s\n",
      "380:\ttest: 0.8688347\tbest: 0.8688347 (380)\ttotal: 1m 11s\tremaining: 1m 56s\n",
      "381:\ttest: 0.8688588\tbest: 0.8688588 (381)\ttotal: 1m 11s\tremaining: 1m 56s\n",
      "382:\ttest: 0.8688705\tbest: 0.8688705 (382)\ttotal: 1m 12s\tremaining: 1m 56s\n",
      "383:\ttest: 0.8689344\tbest: 0.8689344 (383)\ttotal: 1m 12s\tremaining: 1m 55s\n",
      "384:\ttest: 0.8689340\tbest: 0.8689344 (383)\ttotal: 1m 12s\tremaining: 1m 55s\n",
      "385:\ttest: 0.8689448\tbest: 0.8689448 (385)\ttotal: 1m 12s\tremaining: 1m 55s\n",
      "386:\ttest: 0.8689542\tbest: 0.8689542 (386)\ttotal: 1m 12s\tremaining: 1m 55s\n",
      "387:\ttest: 0.8690490\tbest: 0.8690490 (387)\ttotal: 1m 12s\tremaining: 1m 55s\n",
      "388:\ttest: 0.8690347\tbest: 0.8690490 (387)\ttotal: 1m 13s\tremaining: 1m 54s\n",
      "389:\ttest: 0.8690484\tbest: 0.8690490 (387)\ttotal: 1m 13s\tremaining: 1m 54s\n",
      "390:\ttest: 0.8690925\tbest: 0.8690925 (390)\ttotal: 1m 13s\tremaining: 1m 54s\n",
      "391:\ttest: 0.8690731\tbest: 0.8690925 (390)\ttotal: 1m 13s\tremaining: 1m 54s\n",
      "392:\ttest: 0.8690695\tbest: 0.8690925 (390)\ttotal: 1m 13s\tremaining: 1m 54s\n",
      "393:\ttest: 0.8690700\tbest: 0.8690925 (390)\ttotal: 1m 14s\tremaining: 1m 53s\n",
      "394:\ttest: 0.8691355\tbest: 0.8691355 (394)\ttotal: 1m 14s\tremaining: 1m 53s\n",
      "395:\ttest: 0.8691867\tbest: 0.8691867 (395)\ttotal: 1m 14s\tremaining: 1m 53s\n",
      "396:\ttest: 0.8692186\tbest: 0.8692186 (396)\ttotal: 1m 14s\tremaining: 1m 53s\n",
      "397:\ttest: 0.8692527\tbest: 0.8692527 (397)\ttotal: 1m 14s\tremaining: 1m 53s\n",
      "398:\ttest: 0.8692935\tbest: 0.8692935 (398)\ttotal: 1m 14s\tremaining: 1m 52s\n",
      "399:\ttest: 0.8693351\tbest: 0.8693351 (399)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "400:\ttest: 0.8693537\tbest: 0.8693537 (400)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "401:\ttest: 0.8693869\tbest: 0.8693869 (401)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "402:\ttest: 0.8693832\tbest: 0.8693869 (401)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "403:\ttest: 0.8693892\tbest: 0.8693892 (403)\ttotal: 1m 15s\tremaining: 1m 51s\n",
      "404:\ttest: 0.8693911\tbest: 0.8693911 (404)\ttotal: 1m 15s\tremaining: 1m 51s\n",
      "405:\ttest: 0.8694292\tbest: 0.8694292 (405)\ttotal: 1m 16s\tremaining: 1m 51s\n",
      "406:\ttest: 0.8695043\tbest: 0.8695043 (406)\ttotal: 1m 16s\tremaining: 1m 51s\n",
      "407:\ttest: 0.8695011\tbest: 0.8695043 (406)\ttotal: 1m 16s\tremaining: 1m 51s\n",
      "408:\ttest: 0.8695451\tbest: 0.8695451 (408)\ttotal: 1m 16s\tremaining: 1m 50s\n",
      "409:\ttest: 0.8695503\tbest: 0.8695503 (409)\ttotal: 1m 16s\tremaining: 1m 50s\n",
      "410:\ttest: 0.8695663\tbest: 0.8695663 (410)\ttotal: 1m 17s\tremaining: 1m 50s\n",
      "411:\ttest: 0.8695647\tbest: 0.8695663 (410)\ttotal: 1m 17s\tremaining: 1m 50s\n",
      "412:\ttest: 0.8696333\tbest: 0.8696333 (412)\ttotal: 1m 17s\tremaining: 1m 50s\n",
      "413:\ttest: 0.8696181\tbest: 0.8696333 (412)\ttotal: 1m 17s\tremaining: 1m 49s\n",
      "414:\ttest: 0.8696846\tbest: 0.8696846 (414)\ttotal: 1m 17s\tremaining: 1m 49s\n",
      "415:\ttest: 0.8696917\tbest: 0.8696917 (415)\ttotal: 1m 17s\tremaining: 1m 49s\n",
      "416:\ttest: 0.8696795\tbest: 0.8696917 (415)\ttotal: 1m 18s\tremaining: 1m 49s\n",
      "417:\ttest: 0.8696932\tbest: 0.8696932 (417)\ttotal: 1m 18s\tremaining: 1m 49s\n",
      "418:\ttest: 0.8697365\tbest: 0.8697365 (418)\ttotal: 1m 18s\tremaining: 1m 48s\n",
      "419:\ttest: 0.8697271\tbest: 0.8697365 (418)\ttotal: 1m 18s\tremaining: 1m 48s\n",
      "420:\ttest: 0.8697459\tbest: 0.8697459 (420)\ttotal: 1m 18s\tremaining: 1m 48s\n",
      "421:\ttest: 0.8697414\tbest: 0.8697459 (420)\ttotal: 1m 19s\tremaining: 1m 48s\n",
      "422:\ttest: 0.8697616\tbest: 0.8697616 (422)\ttotal: 1m 19s\tremaining: 1m 48s\n",
      "423:\ttest: 0.8698052\tbest: 0.8698052 (423)\ttotal: 1m 19s\tremaining: 1m 47s\n",
      "424:\ttest: 0.8697890\tbest: 0.8698052 (423)\ttotal: 1m 19s\tremaining: 1m 47s\n",
      "425:\ttest: 0.8698064\tbest: 0.8698064 (425)\ttotal: 1m 19s\tremaining: 1m 47s\n",
      "426:\ttest: 0.8698436\tbest: 0.8698436 (426)\ttotal: 1m 19s\tremaining: 1m 47s\n",
      "427:\ttest: 0.8698392\tbest: 0.8698436 (426)\ttotal: 1m 20s\tremaining: 1m 47s\n",
      "428:\ttest: 0.8698620\tbest: 0.8698620 (428)\ttotal: 1m 20s\tremaining: 1m 46s\n",
      "429:\ttest: 0.8698588\tbest: 0.8698620 (428)\ttotal: 1m 20s\tremaining: 1m 46s\n",
      "430:\ttest: 0.8698557\tbest: 0.8698620 (428)\ttotal: 1m 20s\tremaining: 1m 46s\n",
      "431:\ttest: 0.8698830\tbest: 0.8698830 (431)\ttotal: 1m 20s\tremaining: 1m 46s\n",
      "432:\ttest: 0.8698999\tbest: 0.8698999 (432)\ttotal: 1m 20s\tremaining: 1m 46s\n",
      "433:\ttest: 0.8698666\tbest: 0.8698999 (432)\ttotal: 1m 21s\tremaining: 1m 45s\n",
      "434:\ttest: 0.8699240\tbest: 0.8699240 (434)\ttotal: 1m 21s\tremaining: 1m 45s\n",
      "435:\ttest: 0.8699724\tbest: 0.8699724 (435)\ttotal: 1m 21s\tremaining: 1m 45s\n",
      "436:\ttest: 0.8700043\tbest: 0.8700043 (436)\ttotal: 1m 21s\tremaining: 1m 45s\n",
      "437:\ttest: 0.8700909\tbest: 0.8700909 (437)\ttotal: 1m 21s\tremaining: 1m 45s\n",
      "438:\ttest: 0.8701940\tbest: 0.8701940 (438)\ttotal: 1m 22s\tremaining: 1m 44s\n",
      "439:\ttest: 0.8701880\tbest: 0.8701940 (438)\ttotal: 1m 22s\tremaining: 1m 44s\n",
      "440:\ttest: 0.8702281\tbest: 0.8702281 (440)\ttotal: 1m 22s\tremaining: 1m 44s\n",
      "441:\ttest: 0.8702499\tbest: 0.8702499 (441)\ttotal: 1m 22s\tremaining: 1m 44s\n",
      "442:\ttest: 0.8702881\tbest: 0.8702881 (442)\ttotal: 1m 22s\tremaining: 1m 44s\n",
      "443:\ttest: 0.8702938\tbest: 0.8702938 (443)\ttotal: 1m 22s\tremaining: 1m 43s\n",
      "444:\ttest: 0.8703117\tbest: 0.8703117 (444)\ttotal: 1m 23s\tremaining: 1m 43s\n",
      "445:\ttest: 0.8703126\tbest: 0.8703126 (445)\ttotal: 1m 23s\tremaining: 1m 43s\n",
      "446:\ttest: 0.8703252\tbest: 0.8703252 (446)\ttotal: 1m 23s\tremaining: 1m 43s\n",
      "447:\ttest: 0.8703394\tbest: 0.8703394 (447)\ttotal: 1m 23s\tremaining: 1m 43s\n",
      "448:\ttest: 0.8703921\tbest: 0.8703921 (448)\ttotal: 1m 23s\tremaining: 1m 42s\n",
      "449:\ttest: 0.8703943\tbest: 0.8703943 (449)\ttotal: 1m 23s\tremaining: 1m 42s\n",
      "450:\ttest: 0.8703963\tbest: 0.8703963 (450)\ttotal: 1m 24s\tremaining: 1m 42s\n",
      "451:\ttest: 0.8704136\tbest: 0.8704136 (451)\ttotal: 1m 24s\tremaining: 1m 42s\n",
      "452:\ttest: 0.8704576\tbest: 0.8704576 (452)\ttotal: 1m 24s\tremaining: 1m 42s\n",
      "453:\ttest: 0.8704685\tbest: 0.8704685 (453)\ttotal: 1m 24s\tremaining: 1m 41s\n",
      "454:\ttest: 0.8704844\tbest: 0.8704844 (454)\ttotal: 1m 24s\tremaining: 1m 41s\n",
      "455:\ttest: 0.8704987\tbest: 0.8704987 (455)\ttotal: 1m 25s\tremaining: 1m 41s\n",
      "456:\ttest: 0.8704948\tbest: 0.8704987 (455)\ttotal: 1m 25s\tremaining: 1m 41s\n",
      "457:\ttest: 0.8704874\tbest: 0.8704987 (455)\ttotal: 1m 25s\tremaining: 1m 41s\n",
      "458:\ttest: 0.8705060\tbest: 0.8705060 (458)\ttotal: 1m 25s\tremaining: 1m 40s\n",
      "459:\ttest: 0.8706710\tbest: 0.8706710 (459)\ttotal: 1m 25s\tremaining: 1m 40s\n",
      "460:\ttest: 0.8706873\tbest: 0.8706873 (460)\ttotal: 1m 25s\tremaining: 1m 40s\n",
      "461:\ttest: 0.8706964\tbest: 0.8706964 (461)\ttotal: 1m 26s\tremaining: 1m 40s\n",
      "462:\ttest: 0.8708053\tbest: 0.8708053 (462)\ttotal: 1m 26s\tremaining: 1m 40s\n",
      "463:\ttest: 0.8708182\tbest: 0.8708182 (463)\ttotal: 1m 26s\tremaining: 1m 39s\n",
      "464:\ttest: 0.8708410\tbest: 0.8708410 (464)\ttotal: 1m 26s\tremaining: 1m 39s\n",
      "465:\ttest: 0.8708482\tbest: 0.8708482 (465)\ttotal: 1m 26s\tremaining: 1m 39s\n",
      "466:\ttest: 0.8708398\tbest: 0.8708482 (465)\ttotal: 1m 26s\tremaining: 1m 39s\n",
      "467:\ttest: 0.8708481\tbest: 0.8708482 (465)\ttotal: 1m 27s\tremaining: 1m 39s\n",
      "468:\ttest: 0.8708514\tbest: 0.8708514 (468)\ttotal: 1m 27s\tremaining: 1m 38s\n",
      "469:\ttest: 0.8708665\tbest: 0.8708665 (469)\ttotal: 1m 27s\tremaining: 1m 38s\n",
      "470:\ttest: 0.8708660\tbest: 0.8708665 (469)\ttotal: 1m 27s\tremaining: 1m 38s\n",
      "471:\ttest: 0.8708750\tbest: 0.8708750 (471)\ttotal: 1m 27s\tremaining: 1m 38s\n",
      "472:\ttest: 0.8708889\tbest: 0.8708889 (472)\ttotal: 1m 27s\tremaining: 1m 37s\n",
      "473:\ttest: 0.8709260\tbest: 0.8709260 (473)\ttotal: 1m 28s\tremaining: 1m 37s\n",
      "474:\ttest: 0.8710016\tbest: 0.8710016 (474)\ttotal: 1m 28s\tremaining: 1m 37s\n",
      "475:\ttest: 0.8710075\tbest: 0.8710075 (475)\ttotal: 1m 28s\tremaining: 1m 37s\n",
      "476:\ttest: 0.8710311\tbest: 0.8710311 (476)\ttotal: 1m 28s\tremaining: 1m 37s\n",
      "477:\ttest: 0.8710358\tbest: 0.8710358 (477)\ttotal: 1m 28s\tremaining: 1m 37s\n",
      "478:\ttest: 0.8710366\tbest: 0.8710366 (478)\ttotal: 1m 29s\tremaining: 1m 36s\n",
      "479:\ttest: 0.8710355\tbest: 0.8710366 (478)\ttotal: 1m 29s\tremaining: 1m 36s\n",
      "480:\ttest: 0.8710338\tbest: 0.8710366 (478)\ttotal: 1m 29s\tremaining: 1m 36s\n",
      "481:\ttest: 0.8710896\tbest: 0.8710896 (481)\ttotal: 1m 29s\tremaining: 1m 36s\n",
      "482:\ttest: 0.8711278\tbest: 0.8711278 (482)\ttotal: 1m 29s\tremaining: 1m 36s\n",
      "483:\ttest: 0.8711354\tbest: 0.8711354 (483)\ttotal: 1m 29s\tremaining: 1m 35s\n",
      "484:\ttest: 0.8711256\tbest: 0.8711354 (483)\ttotal: 1m 30s\tremaining: 1m 35s\n",
      "485:\ttest: 0.8711191\tbest: 0.8711354 (483)\ttotal: 1m 30s\tremaining: 1m 35s\n",
      "486:\ttest: 0.8712179\tbest: 0.8712179 (486)\ttotal: 1m 30s\tremaining: 1m 35s\n",
      "487:\ttest: 0.8712277\tbest: 0.8712277 (487)\ttotal: 1m 30s\tremaining: 1m 35s\n",
      "488:\ttest: 0.8712279\tbest: 0.8712279 (488)\ttotal: 1m 30s\tremaining: 1m 34s\n",
      "489:\ttest: 0.8712790\tbest: 0.8712790 (489)\ttotal: 1m 30s\tremaining: 1m 34s\n",
      "490:\ttest: 0.8713251\tbest: 0.8713251 (490)\ttotal: 1m 31s\tremaining: 1m 34s\n",
      "491:\ttest: 0.8713814\tbest: 0.8713814 (491)\ttotal: 1m 31s\tremaining: 1m 34s\n",
      "492:\ttest: 0.8714000\tbest: 0.8714000 (492)\ttotal: 1m 31s\tremaining: 1m 34s\n",
      "493:\ttest: 0.8714064\tbest: 0.8714064 (493)\ttotal: 1m 31s\tremaining: 1m 33s\n",
      "494:\ttest: 0.8714058\tbest: 0.8714064 (493)\ttotal: 1m 31s\tremaining: 1m 33s\n",
      "495:\ttest: 0.8714170\tbest: 0.8714170 (495)\ttotal: 1m 31s\tremaining: 1m 33s\n",
      "496:\ttest: 0.8714248\tbest: 0.8714248 (496)\ttotal: 1m 32s\tremaining: 1m 33s\n",
      "497:\ttest: 0.8714180\tbest: 0.8714248 (496)\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "498:\ttest: 0.8714325\tbest: 0.8714325 (498)\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "499:\ttest: 0.8714553\tbest: 0.8714553 (499)\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "500:\ttest: 0.8715293\tbest: 0.8715293 (500)\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "501:\ttest: 0.8715667\tbest: 0.8715667 (501)\ttotal: 1m 32s\tremaining: 1m 32s\n",
      "502:\ttest: 0.8715715\tbest: 0.8715715 (502)\ttotal: 1m 33s\tremaining: 1m 32s\n",
      "503:\ttest: 0.8715739\tbest: 0.8715739 (503)\ttotal: 1m 33s\tremaining: 1m 31s\n",
      "504:\ttest: 0.8715776\tbest: 0.8715776 (504)\ttotal: 1m 33s\tremaining: 1m 31s\n",
      "505:\ttest: 0.8716169\tbest: 0.8716169 (505)\ttotal: 1m 33s\tremaining: 1m 31s\n",
      "506:\ttest: 0.8716364\tbest: 0.8716364 (506)\ttotal: 1m 33s\tremaining: 1m 31s\n",
      "507:\ttest: 0.8716530\tbest: 0.8716530 (507)\ttotal: 1m 34s\tremaining: 1m 31s\n",
      "508:\ttest: 0.8716637\tbest: 0.8716637 (508)\ttotal: 1m 34s\tremaining: 1m 30s\n",
      "509:\ttest: 0.8716605\tbest: 0.8716637 (508)\ttotal: 1m 34s\tremaining: 1m 30s\n",
      "510:\ttest: 0.8716724\tbest: 0.8716724 (510)\ttotal: 1m 34s\tremaining: 1m 30s\n",
      "511:\ttest: 0.8716988\tbest: 0.8716988 (511)\ttotal: 1m 34s\tremaining: 1m 30s\n",
      "512:\ttest: 0.8717162\tbest: 0.8717162 (512)\ttotal: 1m 34s\tremaining: 1m 30s\n",
      "513:\ttest: 0.8718064\tbest: 0.8718064 (513)\ttotal: 1m 35s\tremaining: 1m 29s\n",
      "514:\ttest: 0.8718146\tbest: 0.8718146 (514)\ttotal: 1m 35s\tremaining: 1m 29s\n",
      "515:\ttest: 0.8718803\tbest: 0.8718803 (515)\ttotal: 1m 35s\tremaining: 1m 29s\n",
      "516:\ttest: 0.8718894\tbest: 0.8718894 (516)\ttotal: 1m 35s\tremaining: 1m 29s\n",
      "517:\ttest: 0.8719024\tbest: 0.8719024 (517)\ttotal: 1m 35s\tremaining: 1m 29s\n",
      "518:\ttest: 0.8719122\tbest: 0.8719122 (518)\ttotal: 1m 36s\tremaining: 1m 28s\n",
      "519:\ttest: 0.8719222\tbest: 0.8719222 (519)\ttotal: 1m 36s\tremaining: 1m 28s\n",
      "520:\ttest: 0.8719498\tbest: 0.8719498 (520)\ttotal: 1m 36s\tremaining: 1m 28s\n",
      "521:\ttest: 0.8719340\tbest: 0.8719498 (520)\ttotal: 1m 36s\tremaining: 1m 28s\n",
      "522:\ttest: 0.8719344\tbest: 0.8719498 (520)\ttotal: 1m 36s\tremaining: 1m 28s\n",
      "523:\ttest: 0.8719345\tbest: 0.8719498 (520)\ttotal: 1m 36s\tremaining: 1m 27s\n",
      "524:\ttest: 0.8719368\tbest: 0.8719498 (520)\ttotal: 1m 36s\tremaining: 1m 27s\n",
      "525:\ttest: 0.8719660\tbest: 0.8719660 (525)\ttotal: 1m 37s\tremaining: 1m 27s\n",
      "526:\ttest: 0.8719733\tbest: 0.8719733 (526)\ttotal: 1m 37s\tremaining: 1m 27s\n",
      "527:\ttest: 0.8719718\tbest: 0.8719733 (526)\ttotal: 1m 37s\tremaining: 1m 27s\n",
      "528:\ttest: 0.8719858\tbest: 0.8719858 (528)\ttotal: 1m 37s\tremaining: 1m 26s\n",
      "529:\ttest: 0.8719761\tbest: 0.8719858 (528)\ttotal: 1m 37s\tremaining: 1m 26s\n",
      "530:\ttest: 0.8719828\tbest: 0.8719858 (528)\ttotal: 1m 38s\tremaining: 1m 26s\n",
      "531:\ttest: 0.8719992\tbest: 0.8719992 (531)\ttotal: 1m 38s\tremaining: 1m 26s\n",
      "532:\ttest: 0.8720207\tbest: 0.8720207 (532)\ttotal: 1m 38s\tremaining: 1m 26s\n",
      "533:\ttest: 0.8720369\tbest: 0.8720369 (533)\ttotal: 1m 38s\tremaining: 1m 26s\n",
      "534:\ttest: 0.8720287\tbest: 0.8720369 (533)\ttotal: 1m 38s\tremaining: 1m 25s\n",
      "535:\ttest: 0.8720549\tbest: 0.8720549 (535)\ttotal: 1m 38s\tremaining: 1m 25s\n",
      "536:\ttest: 0.8720608\tbest: 0.8720608 (536)\ttotal: 1m 39s\tremaining: 1m 25s\n",
      "537:\ttest: 0.8720788\tbest: 0.8720788 (537)\ttotal: 1m 39s\tremaining: 1m 25s\n",
      "538:\ttest: 0.8720867\tbest: 0.8720867 (538)\ttotal: 1m 39s\tremaining: 1m 25s\n",
      "539:\ttest: 0.8721342\tbest: 0.8721342 (539)\ttotal: 1m 39s\tremaining: 1m 24s\n",
      "540:\ttest: 0.8721409\tbest: 0.8721409 (540)\ttotal: 1m 39s\tremaining: 1m 24s\n",
      "541:\ttest: 0.8721406\tbest: 0.8721409 (540)\ttotal: 1m 39s\tremaining: 1m 24s\n",
      "542:\ttest: 0.8721916\tbest: 0.8721916 (542)\ttotal: 1m 40s\tremaining: 1m 24s\n",
      "543:\ttest: 0.8722432\tbest: 0.8722432 (543)\ttotal: 1m 40s\tremaining: 1m 24s\n",
      "544:\ttest: 0.8722472\tbest: 0.8722472 (544)\ttotal: 1m 40s\tremaining: 1m 23s\n",
      "545:\ttest: 0.8722606\tbest: 0.8722606 (545)\ttotal: 1m 40s\tremaining: 1m 23s\n",
      "546:\ttest: 0.8722629\tbest: 0.8722629 (546)\ttotal: 1m 40s\tremaining: 1m 23s\n",
      "547:\ttest: 0.8722557\tbest: 0.8722629 (546)\ttotal: 1m 40s\tremaining: 1m 23s\n",
      "548:\ttest: 0.8722642\tbest: 0.8722642 (548)\ttotal: 1m 41s\tremaining: 1m 23s\n",
      "549:\ttest: 0.8722754\tbest: 0.8722754 (549)\ttotal: 1m 41s\tremaining: 1m 22s\n",
      "550:\ttest: 0.8722733\tbest: 0.8722754 (549)\ttotal: 1m 41s\tremaining: 1m 22s\n",
      "551:\ttest: 0.8722875\tbest: 0.8722875 (551)\ttotal: 1m 41s\tremaining: 1m 22s\n",
      "552:\ttest: 0.8723101\tbest: 0.8723101 (552)\ttotal: 1m 41s\tremaining: 1m 22s\n",
      "553:\ttest: 0.8723192\tbest: 0.8723192 (553)\ttotal: 1m 42s\tremaining: 1m 22s\n",
      "554:\ttest: 0.8723449\tbest: 0.8723449 (554)\ttotal: 1m 42s\tremaining: 1m 21s\n",
      "555:\ttest: 0.8723441\tbest: 0.8723449 (554)\ttotal: 1m 42s\tremaining: 1m 21s\n",
      "556:\ttest: 0.8723504\tbest: 0.8723504 (556)\ttotal: 1m 42s\tremaining: 1m 21s\n",
      "557:\ttest: 0.8724161\tbest: 0.8724161 (557)\ttotal: 1m 42s\tremaining: 1m 21s\n",
      "558:\ttest: 0.8724220\tbest: 0.8724220 (558)\ttotal: 1m 42s\tremaining: 1m 21s\n",
      "559:\ttest: 0.8724385\tbest: 0.8724385 (559)\ttotal: 1m 43s\tremaining: 1m 21s\n",
      "560:\ttest: 0.8724638\tbest: 0.8724638 (560)\ttotal: 1m 43s\tremaining: 1m 20s\n",
      "561:\ttest: 0.8724721\tbest: 0.8724721 (561)\ttotal: 1m 43s\tremaining: 1m 20s\n",
      "562:\ttest: 0.8724926\tbest: 0.8724926 (562)\ttotal: 1m 43s\tremaining: 1m 20s\n",
      "563:\ttest: 0.8725220\tbest: 0.8725220 (563)\ttotal: 1m 43s\tremaining: 1m 20s\n",
      "564:\ttest: 0.8725357\tbest: 0.8725357 (564)\ttotal: 1m 44s\tremaining: 1m 20s\n",
      "565:\ttest: 0.8725378\tbest: 0.8725378 (565)\ttotal: 1m 44s\tremaining: 1m 19s\n",
      "566:\ttest: 0.8725429\tbest: 0.8725429 (566)\ttotal: 1m 44s\tremaining: 1m 19s\n",
      "567:\ttest: 0.8726457\tbest: 0.8726457 (567)\ttotal: 1m 44s\tremaining: 1m 19s\n",
      "568:\ttest: 0.8726677\tbest: 0.8726677 (568)\ttotal: 1m 44s\tremaining: 1m 19s\n",
      "569:\ttest: 0.8726684\tbest: 0.8726684 (569)\ttotal: 1m 44s\tremaining: 1m 19s\n",
      "570:\ttest: 0.8726708\tbest: 0.8726708 (570)\ttotal: 1m 45s\tremaining: 1m 18s\n",
      "571:\ttest: 0.8726763\tbest: 0.8726763 (571)\ttotal: 1m 45s\tremaining: 1m 18s\n",
      "572:\ttest: 0.8726845\tbest: 0.8726845 (572)\ttotal: 1m 45s\tremaining: 1m 18s\n",
      "573:\ttest: 0.8726926\tbest: 0.8726926 (573)\ttotal: 1m 45s\tremaining: 1m 18s\n",
      "574:\ttest: 0.8726952\tbest: 0.8726952 (574)\ttotal: 1m 45s\tremaining: 1m 18s\n",
      "575:\ttest: 0.8727155\tbest: 0.8727155 (575)\ttotal: 1m 45s\tremaining: 1m 17s\n",
      "576:\ttest: 0.8727295\tbest: 0.8727295 (576)\ttotal: 1m 46s\tremaining: 1m 17s\n",
      "577:\ttest: 0.8727797\tbest: 0.8727797 (577)\ttotal: 1m 46s\tremaining: 1m 17s\n",
      "578:\ttest: 0.8727763\tbest: 0.8727797 (577)\ttotal: 1m 46s\tremaining: 1m 17s\n",
      "579:\ttest: 0.8727845\tbest: 0.8727845 (579)\ttotal: 1m 46s\tremaining: 1m 17s\n",
      "580:\ttest: 0.8728118\tbest: 0.8728118 (580)\ttotal: 1m 46s\tremaining: 1m 17s\n",
      "581:\ttest: 0.8728112\tbest: 0.8728118 (580)\ttotal: 1m 46s\tremaining: 1m 16s\n",
      "582:\ttest: 0.8728309\tbest: 0.8728309 (582)\ttotal: 1m 47s\tremaining: 1m 16s\n",
      "583:\ttest: 0.8728487\tbest: 0.8728487 (583)\ttotal: 1m 47s\tremaining: 1m 16s\n",
      "584:\ttest: 0.8728749\tbest: 0.8728749 (584)\ttotal: 1m 47s\tremaining: 1m 16s\n",
      "585:\ttest: 0.8728970\tbest: 0.8728970 (585)\ttotal: 1m 47s\tremaining: 1m 16s\n",
      "586:\ttest: 0.8728984\tbest: 0.8728984 (586)\ttotal: 1m 47s\tremaining: 1m 15s\n",
      "587:\ttest: 0.8729132\tbest: 0.8729132 (587)\ttotal: 1m 48s\tremaining: 1m 15s\n",
      "588:\ttest: 0.8729648\tbest: 0.8729648 (588)\ttotal: 1m 48s\tremaining: 1m 15s\n",
      "589:\ttest: 0.8729930\tbest: 0.8729930 (589)\ttotal: 1m 48s\tremaining: 1m 15s\n",
      "590:\ttest: 0.8730182\tbest: 0.8730182 (590)\ttotal: 1m 48s\tremaining: 1m 15s\n",
      "591:\ttest: 0.8729312\tbest: 0.8730182 (590)\ttotal: 1m 48s\tremaining: 1m 14s\n",
      "592:\ttest: 0.8729345\tbest: 0.8730182 (590)\ttotal: 1m 48s\tremaining: 1m 14s\n",
      "593:\ttest: 0.8729343\tbest: 0.8730182 (590)\ttotal: 1m 49s\tremaining: 1m 14s\n",
      "594:\ttest: 0.8729308\tbest: 0.8730182 (590)\ttotal: 1m 49s\tremaining: 1m 14s\n",
      "595:\ttest: 0.8729472\tbest: 0.8730182 (590)\ttotal: 1m 49s\tremaining: 1m 14s\n",
      "596:\ttest: 0.8729625\tbest: 0.8730182 (590)\ttotal: 1m 49s\tremaining: 1m 14s\n",
      "597:\ttest: 0.8729766\tbest: 0.8730182 (590)\ttotal: 1m 49s\tremaining: 1m 13s\n",
      "598:\ttest: 0.8729751\tbest: 0.8730182 (590)\ttotal: 1m 50s\tremaining: 1m 13s\n",
      "599:\ttest: 0.8730045\tbest: 0.8730182 (590)\ttotal: 1m 50s\tremaining: 1m 13s\n",
      "600:\ttest: 0.8730299\tbest: 0.8730299 (600)\ttotal: 1m 50s\tremaining: 1m 13s\n",
      "601:\ttest: 0.8730540\tbest: 0.8730540 (601)\ttotal: 1m 50s\tremaining: 1m 13s\n",
      "602:\ttest: 0.8730649\tbest: 0.8730649 (602)\ttotal: 1m 50s\tremaining: 1m 12s\n",
      "603:\ttest: 0.8730750\tbest: 0.8730750 (603)\ttotal: 1m 50s\tremaining: 1m 12s\n",
      "604:\ttest: 0.8730767\tbest: 0.8730767 (604)\ttotal: 1m 51s\tremaining: 1m 12s\n",
      "605:\ttest: 0.8731097\tbest: 0.8731097 (605)\ttotal: 1m 51s\tremaining: 1m 12s\n",
      "606:\ttest: 0.8730990\tbest: 0.8731097 (605)\ttotal: 1m 51s\tremaining: 1m 12s\n",
      "607:\ttest: 0.8731178\tbest: 0.8731178 (607)\ttotal: 1m 51s\tremaining: 1m 11s\n",
      "608:\ttest: 0.8731382\tbest: 0.8731382 (608)\ttotal: 1m 51s\tremaining: 1m 11s\n",
      "609:\ttest: 0.8731407\tbest: 0.8731407 (609)\ttotal: 1m 51s\tremaining: 1m 11s\n",
      "610:\ttest: 0.8731581\tbest: 0.8731581 (610)\ttotal: 1m 52s\tremaining: 1m 11s\n",
      "611:\ttest: 0.8731778\tbest: 0.8731778 (611)\ttotal: 1m 52s\tremaining: 1m 11s\n",
      "612:\ttest: 0.8731746\tbest: 0.8731778 (611)\ttotal: 1m 52s\tremaining: 1m 11s\n",
      "613:\ttest: 0.8731646\tbest: 0.8731778 (611)\ttotal: 1m 52s\tremaining: 1m 10s\n",
      "614:\ttest: 0.8731829\tbest: 0.8731829 (614)\ttotal: 1m 52s\tremaining: 1m 10s\n",
      "615:\ttest: 0.8731871\tbest: 0.8731871 (615)\ttotal: 1m 52s\tremaining: 1m 10s\n",
      "616:\ttest: 0.8731802\tbest: 0.8731871 (615)\ttotal: 1m 53s\tremaining: 1m 10s\n",
      "617:\ttest: 0.8731851\tbest: 0.8731871 (615)\ttotal: 1m 53s\tremaining: 1m 10s\n",
      "618:\ttest: 0.8731958\tbest: 0.8731958 (618)\ttotal: 1m 53s\tremaining: 1m 9s\n",
      "619:\ttest: 0.8732058\tbest: 0.8732058 (619)\ttotal: 1m 53s\tremaining: 1m 9s\n",
      "620:\ttest: 0.8732075\tbest: 0.8732075 (620)\ttotal: 1m 53s\tremaining: 1m 9s\n",
      "621:\ttest: 0.8732239\tbest: 0.8732239 (621)\ttotal: 1m 54s\tremaining: 1m 9s\n",
      "622:\ttest: 0.8732257\tbest: 0.8732257 (622)\ttotal: 1m 54s\tremaining: 1m 9s\n",
      "623:\ttest: 0.8732289\tbest: 0.8732289 (623)\ttotal: 1m 54s\tremaining: 1m 8s\n",
      "624:\ttest: 0.8732474\tbest: 0.8732474 (624)\ttotal: 1m 54s\tremaining: 1m 8s\n",
      "625:\ttest: 0.8732612\tbest: 0.8732612 (625)\ttotal: 1m 54s\tremaining: 1m 8s\n",
      "626:\ttest: 0.8732626\tbest: 0.8732626 (626)\ttotal: 1m 54s\tremaining: 1m 8s\n",
      "627:\ttest: 0.8732864\tbest: 0.8732864 (627)\ttotal: 1m 55s\tremaining: 1m 8s\n",
      "628:\ttest: 0.8732919\tbest: 0.8732919 (628)\ttotal: 1m 55s\tremaining: 1m 7s\n",
      "629:\ttest: 0.8732959\tbest: 0.8732959 (629)\ttotal: 1m 55s\tremaining: 1m 7s\n",
      "630:\ttest: 0.8732991\tbest: 0.8732991 (630)\ttotal: 1m 55s\tremaining: 1m 7s\n",
      "631:\ttest: 0.8733099\tbest: 0.8733099 (631)\ttotal: 1m 55s\tremaining: 1m 7s\n",
      "632:\ttest: 0.8733088\tbest: 0.8733099 (631)\ttotal: 1m 55s\tremaining: 1m 7s\n",
      "633:\ttest: 0.8733255\tbest: 0.8733255 (633)\ttotal: 1m 56s\tremaining: 1m 7s\n",
      "634:\ttest: 0.8733343\tbest: 0.8733343 (634)\ttotal: 1m 56s\tremaining: 1m 6s\n",
      "635:\ttest: 0.8733845\tbest: 0.8733845 (635)\ttotal: 1m 56s\tremaining: 1m 6s\n",
      "636:\ttest: 0.8734075\tbest: 0.8734075 (636)\ttotal: 1m 56s\tremaining: 1m 6s\n",
      "637:\ttest: 0.8734585\tbest: 0.8734585 (637)\ttotal: 1m 56s\tremaining: 1m 6s\n",
      "638:\ttest: 0.8734407\tbest: 0.8734585 (637)\ttotal: 1m 57s\tremaining: 1m 6s\n",
      "639:\ttest: 0.8734375\tbest: 0.8734585 (637)\ttotal: 1m 57s\tremaining: 1m 5s\n",
      "640:\ttest: 0.8734475\tbest: 0.8734585 (637)\ttotal: 1m 57s\tremaining: 1m 5s\n",
      "641:\ttest: 0.8734606\tbest: 0.8734606 (641)\ttotal: 1m 57s\tremaining: 1m 5s\n",
      "642:\ttest: 0.8734693\tbest: 0.8734693 (642)\ttotal: 1m 57s\tremaining: 1m 5s\n",
      "643:\ttest: 0.8734760\tbest: 0.8734760 (643)\ttotal: 1m 57s\tremaining: 1m 5s\n",
      "644:\ttest: 0.8734791\tbest: 0.8734791 (644)\ttotal: 1m 58s\tremaining: 1m 4s\n",
      "645:\ttest: 0.8734901\tbest: 0.8734901 (645)\ttotal: 1m 58s\tremaining: 1m 4s\n",
      "646:\ttest: 0.8734954\tbest: 0.8734954 (646)\ttotal: 1m 58s\tremaining: 1m 4s\n",
      "647:\ttest: 0.8735023\tbest: 0.8735023 (647)\ttotal: 1m 58s\tremaining: 1m 4s\n",
      "648:\ttest: 0.8734943\tbest: 0.8735023 (647)\ttotal: 1m 58s\tremaining: 1m 4s\n",
      "649:\ttest: 0.8735537\tbest: 0.8735537 (649)\ttotal: 1m 58s\tremaining: 1m 4s\n",
      "650:\ttest: 0.8735466\tbest: 0.8735537 (649)\ttotal: 1m 59s\tremaining: 1m 3s\n",
      "651:\ttest: 0.8735974\tbest: 0.8735974 (651)\ttotal: 1m 59s\tremaining: 1m 3s\n",
      "652:\ttest: 0.8735966\tbest: 0.8735974 (651)\ttotal: 1m 59s\tremaining: 1m 3s\n",
      "653:\ttest: 0.8736098\tbest: 0.8736098 (653)\ttotal: 1m 59s\tremaining: 1m 3s\n",
      "654:\ttest: 0.8736182\tbest: 0.8736182 (654)\ttotal: 1m 59s\tremaining: 1m 3s\n",
      "655:\ttest: 0.8735953\tbest: 0.8736182 (654)\ttotal: 2m\tremaining: 1m 2s\n",
      "656:\ttest: 0.8736476\tbest: 0.8736476 (656)\ttotal: 2m\tremaining: 1m 2s\n",
      "657:\ttest: 0.8736496\tbest: 0.8736496 (657)\ttotal: 2m\tremaining: 1m 2s\n",
      "658:\ttest: 0.8736552\tbest: 0.8736552 (658)\ttotal: 2m\tremaining: 1m 2s\n",
      "659:\ttest: 0.8736473\tbest: 0.8736552 (658)\ttotal: 2m\tremaining: 1m 2s\n",
      "660:\ttest: 0.8736521\tbest: 0.8736552 (658)\ttotal: 2m\tremaining: 1m 1s\n",
      "661:\ttest: 0.8736557\tbest: 0.8736557 (661)\ttotal: 2m 1s\tremaining: 1m 1s\n",
      "662:\ttest: 0.8737327\tbest: 0.8737327 (662)\ttotal: 2m 1s\tremaining: 1m 1s\n",
      "663:\ttest: 0.8737370\tbest: 0.8737370 (663)\ttotal: 2m 1s\tremaining: 1m 1s\n",
      "664:\ttest: 0.8737544\tbest: 0.8737544 (664)\ttotal: 2m 1s\tremaining: 1m 1s\n",
      "665:\ttest: 0.8737762\tbest: 0.8737762 (665)\ttotal: 2m 1s\tremaining: 1m 1s\n",
      "666:\ttest: 0.8737905\tbest: 0.8737905 (666)\ttotal: 2m 1s\tremaining: 1m\n",
      "667:\ttest: 0.8737908\tbest: 0.8737908 (667)\ttotal: 2m 2s\tremaining: 1m\n",
      "668:\ttest: 0.8738115\tbest: 0.8738115 (668)\ttotal: 2m 2s\tremaining: 1m\n",
      "669:\ttest: 0.8738188\tbest: 0.8738188 (669)\ttotal: 2m 2s\tremaining: 1m\n",
      "670:\ttest: 0.8738216\tbest: 0.8738216 (670)\ttotal: 2m 2s\tremaining: 1m\n",
      "671:\ttest: 0.8738116\tbest: 0.8738216 (670)\ttotal: 2m 2s\tremaining: 59.9s\n",
      "672:\ttest: 0.8738169\tbest: 0.8738216 (670)\ttotal: 2m 2s\tremaining: 59.7s\n",
      "673:\ttest: 0.8738058\tbest: 0.8738216 (670)\ttotal: 2m 3s\tremaining: 59.5s\n",
      "674:\ttest: 0.8738066\tbest: 0.8738216 (670)\ttotal: 2m 3s\tremaining: 59.3s\n",
      "675:\ttest: 0.8738373\tbest: 0.8738373 (675)\ttotal: 2m 3s\tremaining: 59.2s\n",
      "676:\ttest: 0.8738217\tbest: 0.8738373 (675)\ttotal: 2m 3s\tremaining: 59s\n",
      "677:\ttest: 0.8738190\tbest: 0.8738373 (675)\ttotal: 2m 3s\tremaining: 58.8s\n",
      "678:\ttest: 0.8738557\tbest: 0.8738557 (678)\ttotal: 2m 3s\tremaining: 58.6s\n",
      "679:\ttest: 0.8738426\tbest: 0.8738557 (678)\ttotal: 2m 4s\tremaining: 58.4s\n",
      "680:\ttest: 0.8738606\tbest: 0.8738606 (680)\ttotal: 2m 4s\tremaining: 58.2s\n",
      "681:\ttest: 0.8738989\tbest: 0.8738989 (681)\ttotal: 2m 4s\tremaining: 58s\n",
      "682:\ttest: 0.8739188\tbest: 0.8739188 (682)\ttotal: 2m 4s\tremaining: 57.8s\n",
      "683:\ttest: 0.8739236\tbest: 0.8739236 (683)\ttotal: 2m 4s\tremaining: 57.7s\n",
      "684:\ttest: 0.8739326\tbest: 0.8739326 (684)\ttotal: 2m 4s\tremaining: 57.5s\n",
      "685:\ttest: 0.8739354\tbest: 0.8739354 (685)\ttotal: 2m 5s\tremaining: 57.3s\n",
      "686:\ttest: 0.8739505\tbest: 0.8739505 (686)\ttotal: 2m 5s\tremaining: 57.1s\n",
      "687:\ttest: 0.8739421\tbest: 0.8739505 (686)\ttotal: 2m 5s\tremaining: 56.9s\n",
      "688:\ttest: 0.8739571\tbest: 0.8739571 (688)\ttotal: 2m 5s\tremaining: 56.8s\n",
      "689:\ttest: 0.8739762\tbest: 0.8739762 (689)\ttotal: 2m 5s\tremaining: 56.6s\n",
      "690:\ttest: 0.8739796\tbest: 0.8739796 (690)\ttotal: 2m 6s\tremaining: 56.4s\n",
      "691:\ttest: 0.8739785\tbest: 0.8739796 (690)\ttotal: 2m 6s\tremaining: 56.2s\n",
      "692:\ttest: 0.8739853\tbest: 0.8739853 (692)\ttotal: 2m 6s\tremaining: 56s\n",
      "693:\ttest: 0.8740119\tbest: 0.8740119 (693)\ttotal: 2m 6s\tremaining: 55.8s\n",
      "694:\ttest: 0.8740103\tbest: 0.8740119 (693)\ttotal: 2m 6s\tremaining: 55.6s\n",
      "695:\ttest: 0.8740149\tbest: 0.8740149 (695)\ttotal: 2m 6s\tremaining: 55.5s\n",
      "696:\ttest: 0.8740195\tbest: 0.8740195 (696)\ttotal: 2m 7s\tremaining: 55.3s\n",
      "697:\ttest: 0.8740360\tbest: 0.8740360 (697)\ttotal: 2m 7s\tremaining: 55.1s\n",
      "698:\ttest: 0.8740351\tbest: 0.8740360 (697)\ttotal: 2m 7s\tremaining: 54.9s\n",
      "699:\ttest: 0.8740171\tbest: 0.8740360 (697)\ttotal: 2m 7s\tremaining: 54.7s\n",
      "700:\ttest: 0.8740503\tbest: 0.8740503 (700)\ttotal: 2m 7s\tremaining: 54.5s\n",
      "701:\ttest: 0.8740708\tbest: 0.8740708 (701)\ttotal: 2m 8s\tremaining: 54.3s\n",
      "702:\ttest: 0.8741050\tbest: 0.8741050 (702)\ttotal: 2m 8s\tremaining: 54.2s\n",
      "703:\ttest: 0.8740883\tbest: 0.8741050 (702)\ttotal: 2m 8s\tremaining: 54s\n",
      "704:\ttest: 0.8741880\tbest: 0.8741880 (704)\ttotal: 2m 8s\tremaining: 53.8s\n",
      "705:\ttest: 0.8741778\tbest: 0.8741880 (704)\ttotal: 2m 8s\tremaining: 53.6s\n",
      "706:\ttest: 0.8741802\tbest: 0.8741880 (704)\ttotal: 2m 8s\tremaining: 53.4s\n",
      "707:\ttest: 0.8741982\tbest: 0.8741982 (707)\ttotal: 2m 9s\tremaining: 53.2s\n",
      "708:\ttest: 0.8741919\tbest: 0.8741982 (707)\ttotal: 2m 9s\tremaining: 53s\n",
      "709:\ttest: 0.8741987\tbest: 0.8741987 (709)\ttotal: 2m 9s\tremaining: 52.8s\n",
      "710:\ttest: 0.8742190\tbest: 0.8742190 (710)\ttotal: 2m 9s\tremaining: 52.7s\n",
      "711:\ttest: 0.8742262\tbest: 0.8742262 (711)\ttotal: 2m 9s\tremaining: 52.5s\n",
      "712:\ttest: 0.8742165\tbest: 0.8742262 (711)\ttotal: 2m 9s\tremaining: 52.3s\n",
      "713:\ttest: 0.8742369\tbest: 0.8742369 (713)\ttotal: 2m 10s\tremaining: 52.1s\n",
      "714:\ttest: 0.8742454\tbest: 0.8742454 (714)\ttotal: 2m 10s\tremaining: 51.9s\n",
      "715:\ttest: 0.8742442\tbest: 0.8742454 (714)\ttotal: 2m 10s\tremaining: 51.7s\n",
      "716:\ttest: 0.8742511\tbest: 0.8742511 (716)\ttotal: 2m 10s\tremaining: 51.5s\n",
      "717:\ttest: 0.8742632\tbest: 0.8742632 (717)\ttotal: 2m 10s\tremaining: 51.3s\n",
      "718:\ttest: 0.8742682\tbest: 0.8742682 (718)\ttotal: 2m 10s\tremaining: 51.2s\n",
      "719:\ttest: 0.8742462\tbest: 0.8742682 (718)\ttotal: 2m 11s\tremaining: 51s\n",
      "720:\ttest: 0.8742676\tbest: 0.8742682 (718)\ttotal: 2m 11s\tremaining: 50.8s\n",
      "721:\ttest: 0.8742643\tbest: 0.8742682 (718)\ttotal: 2m 11s\tremaining: 50.6s\n",
      "722:\ttest: 0.8742590\tbest: 0.8742682 (718)\ttotal: 2m 11s\tremaining: 50.4s\n",
      "723:\ttest: 0.8742776\tbest: 0.8742776 (723)\ttotal: 2m 11s\tremaining: 50.2s\n",
      "724:\ttest: 0.8742761\tbest: 0.8742776 (723)\ttotal: 2m 11s\tremaining: 50s\n",
      "725:\ttest: 0.8742975\tbest: 0.8742975 (725)\ttotal: 2m 12s\tremaining: 49.9s\n",
      "726:\ttest: 0.8742913\tbest: 0.8742975 (725)\ttotal: 2m 12s\tremaining: 49.7s\n",
      "727:\ttest: 0.8742959\tbest: 0.8742975 (725)\ttotal: 2m 12s\tremaining: 49.5s\n",
      "728:\ttest: 0.8742931\tbest: 0.8742975 (725)\ttotal: 2m 12s\tremaining: 49.3s\n",
      "729:\ttest: 0.8742985\tbest: 0.8742985 (729)\ttotal: 2m 12s\tremaining: 49.1s\n",
      "730:\ttest: 0.8743041\tbest: 0.8743041 (730)\ttotal: 2m 12s\tremaining: 48.9s\n",
      "731:\ttest: 0.8743024\tbest: 0.8743041 (730)\ttotal: 2m 13s\tremaining: 48.7s\n",
      "732:\ttest: 0.8743034\tbest: 0.8743041 (730)\ttotal: 2m 13s\tremaining: 48.6s\n",
      "733:\ttest: 0.8743075\tbest: 0.8743075 (733)\ttotal: 2m 13s\tremaining: 48.4s\n",
      "734:\ttest: 0.8743153\tbest: 0.8743153 (734)\ttotal: 2m 13s\tremaining: 48.2s\n",
      "735:\ttest: 0.8743531\tbest: 0.8743531 (735)\ttotal: 2m 13s\tremaining: 48s\n",
      "736:\ttest: 0.8743617\tbest: 0.8743617 (736)\ttotal: 2m 13s\tremaining: 47.8s\n",
      "737:\ttest: 0.8743628\tbest: 0.8743628 (737)\ttotal: 2m 14s\tremaining: 47.6s\n",
      "738:\ttest: 0.8743593\tbest: 0.8743628 (737)\ttotal: 2m 14s\tremaining: 47.4s\n",
      "739:\ttest: 0.8743518\tbest: 0.8743628 (737)\ttotal: 2m 14s\tremaining: 47.3s\n",
      "740:\ttest: 0.8743393\tbest: 0.8743628 (737)\ttotal: 2m 14s\tremaining: 47.1s\n",
      "741:\ttest: 0.8743427\tbest: 0.8743628 (737)\ttotal: 2m 14s\tremaining: 46.9s\n",
      "742:\ttest: 0.8743287\tbest: 0.8743628 (737)\ttotal: 2m 15s\tremaining: 46.7s\n",
      "743:\ttest: 0.8743324\tbest: 0.8743628 (737)\ttotal: 2m 15s\tremaining: 46.5s\n",
      "744:\ttest: 0.8743410\tbest: 0.8743628 (737)\ttotal: 2m 15s\tremaining: 46.3s\n",
      "745:\ttest: 0.8744534\tbest: 0.8744534 (745)\ttotal: 2m 15s\tremaining: 46.2s\n",
      "746:\ttest: 0.8744536\tbest: 0.8744536 (746)\ttotal: 2m 15s\tremaining: 46s\n",
      "747:\ttest: 0.8744642\tbest: 0.8744642 (747)\ttotal: 2m 15s\tremaining: 45.8s\n",
      "748:\ttest: 0.8744633\tbest: 0.8744642 (747)\ttotal: 2m 16s\tremaining: 45.6s\n",
      "749:\ttest: 0.8744661\tbest: 0.8744661 (749)\ttotal: 2m 16s\tremaining: 45.4s\n",
      "750:\ttest: 0.8745146\tbest: 0.8745146 (750)\ttotal: 2m 16s\tremaining: 45.2s\n",
      "751:\ttest: 0.8745202\tbest: 0.8745202 (751)\ttotal: 2m 16s\tremaining: 45s\n",
      "752:\ttest: 0.8745427\tbest: 0.8745427 (752)\ttotal: 2m 16s\tremaining: 44.9s\n",
      "753:\ttest: 0.8745634\tbest: 0.8745634 (753)\ttotal: 2m 16s\tremaining: 44.7s\n",
      "754:\ttest: 0.8746097\tbest: 0.8746097 (754)\ttotal: 2m 17s\tremaining: 44.5s\n",
      "755:\ttest: 0.8746065\tbest: 0.8746097 (754)\ttotal: 2m 17s\tremaining: 44.3s\n",
      "756:\ttest: 0.8746130\tbest: 0.8746130 (756)\ttotal: 2m 17s\tremaining: 44.1s\n",
      "757:\ttest: 0.8746096\tbest: 0.8746130 (756)\ttotal: 2m 17s\tremaining: 43.9s\n",
      "758:\ttest: 0.8746194\tbest: 0.8746194 (758)\ttotal: 2m 17s\tremaining: 43.7s\n",
      "759:\ttest: 0.8746072\tbest: 0.8746194 (758)\ttotal: 2m 17s\tremaining: 43.6s\n",
      "760:\ttest: 0.8745959\tbest: 0.8746194 (758)\ttotal: 2m 18s\tremaining: 43.4s\n",
      "761:\ttest: 0.8746088\tbest: 0.8746194 (758)\ttotal: 2m 18s\tremaining: 43.2s\n",
      "762:\ttest: 0.8746273\tbest: 0.8746273 (762)\ttotal: 2m 18s\tremaining: 43s\n",
      "763:\ttest: 0.8746165\tbest: 0.8746273 (762)\ttotal: 2m 18s\tremaining: 42.8s\n",
      "764:\ttest: 0.8746083\tbest: 0.8746273 (762)\ttotal: 2m 18s\tremaining: 42.6s\n",
      "765:\ttest: 0.8746096\tbest: 0.8746273 (762)\ttotal: 2m 18s\tremaining: 42.4s\n",
      "766:\ttest: 0.8746569\tbest: 0.8746569 (766)\ttotal: 2m 19s\tremaining: 42.3s\n",
      "767:\ttest: 0.8746540\tbest: 0.8746569 (766)\ttotal: 2m 19s\tremaining: 42.1s\n",
      "768:\ttest: 0.8746356\tbest: 0.8746569 (766)\ttotal: 2m 19s\tremaining: 41.9s\n",
      "769:\ttest: 0.8746421\tbest: 0.8746569 (766)\ttotal: 2m 19s\tremaining: 41.7s\n",
      "770:\ttest: 0.8746506\tbest: 0.8746569 (766)\ttotal: 2m 19s\tremaining: 41.5s\n",
      "771:\ttest: 0.8746531\tbest: 0.8746569 (766)\ttotal: 2m 19s\tremaining: 41.3s\n",
      "772:\ttest: 0.8747071\tbest: 0.8747071 (772)\ttotal: 2m 20s\tremaining: 41.1s\n",
      "773:\ttest: 0.8747169\tbest: 0.8747169 (773)\ttotal: 2m 20s\tremaining: 41s\n",
      "774:\ttest: 0.8747067\tbest: 0.8747169 (773)\ttotal: 2m 20s\tremaining: 40.8s\n",
      "775:\ttest: 0.8747146\tbest: 0.8747169 (773)\ttotal: 2m 20s\tremaining: 40.6s\n",
      "776:\ttest: 0.8747198\tbest: 0.8747198 (776)\ttotal: 2m 20s\tremaining: 40.4s\n",
      "777:\ttest: 0.8747270\tbest: 0.8747270 (777)\ttotal: 2m 20s\tremaining: 40.2s\n",
      "778:\ttest: 0.8747468\tbest: 0.8747468 (778)\ttotal: 2m 21s\tremaining: 40s\n",
      "779:\ttest: 0.8747643\tbest: 0.8747643 (779)\ttotal: 2m 21s\tremaining: 39.9s\n",
      "780:\ttest: 0.8747948\tbest: 0.8747948 (780)\ttotal: 2m 21s\tremaining: 39.7s\n",
      "781:\ttest: 0.8748029\tbest: 0.8748029 (781)\ttotal: 2m 21s\tremaining: 39.5s\n",
      "782:\ttest: 0.8748104\tbest: 0.8748104 (782)\ttotal: 2m 21s\tremaining: 39.3s\n",
      "783:\ttest: 0.8748149\tbest: 0.8748149 (783)\ttotal: 2m 21s\tremaining: 39.1s\n",
      "784:\ttest: 0.8748062\tbest: 0.8748149 (783)\ttotal: 2m 22s\tremaining: 38.9s\n",
      "785:\ttest: 0.8748213\tbest: 0.8748213 (785)\ttotal: 2m 22s\tremaining: 38.8s\n",
      "786:\ttest: 0.8748278\tbest: 0.8748278 (786)\ttotal: 2m 22s\tremaining: 38.6s\n",
      "787:\ttest: 0.8748656\tbest: 0.8748656 (787)\ttotal: 2m 22s\tremaining: 38.4s\n",
      "788:\ttest: 0.8748709\tbest: 0.8748709 (788)\ttotal: 2m 22s\tremaining: 38.2s\n",
      "789:\ttest: 0.8749060\tbest: 0.8749060 (789)\ttotal: 2m 23s\tremaining: 38s\n",
      "790:\ttest: 0.8748862\tbest: 0.8749060 (789)\ttotal: 2m 23s\tremaining: 37.8s\n",
      "791:\ttest: 0.8749012\tbest: 0.8749060 (789)\ttotal: 2m 23s\tremaining: 37.6s\n",
      "792:\ttest: 0.8748998\tbest: 0.8749060 (789)\ttotal: 2m 23s\tremaining: 37.5s\n",
      "793:\ttest: 0.8749102\tbest: 0.8749102 (793)\ttotal: 2m 23s\tremaining: 37.3s\n",
      "794:\ttest: 0.8749222\tbest: 0.8749222 (794)\ttotal: 2m 23s\tremaining: 37.1s\n",
      "795:\ttest: 0.8749237\tbest: 0.8749237 (795)\ttotal: 2m 24s\tremaining: 36.9s\n",
      "796:\ttest: 0.8749284\tbest: 0.8749284 (796)\ttotal: 2m 24s\tremaining: 36.7s\n",
      "797:\ttest: 0.8749202\tbest: 0.8749284 (796)\ttotal: 2m 24s\tremaining: 36.5s\n",
      "798:\ttest: 0.8749234\tbest: 0.8749284 (796)\ttotal: 2m 24s\tremaining: 36.4s\n",
      "799:\ttest: 0.8749377\tbest: 0.8749377 (799)\ttotal: 2m 24s\tremaining: 36.2s\n",
      "800:\ttest: 0.8749588\tbest: 0.8749588 (800)\ttotal: 2m 24s\tremaining: 36s\n",
      "801:\ttest: 0.8749401\tbest: 0.8749588 (800)\ttotal: 2m 25s\tremaining: 35.8s\n",
      "802:\ttest: 0.8749242\tbest: 0.8749588 (800)\ttotal: 2m 25s\tremaining: 35.6s\n",
      "803:\ttest: 0.8749256\tbest: 0.8749588 (800)\ttotal: 2m 25s\tremaining: 35.4s\n",
      "804:\ttest: 0.8749387\tbest: 0.8749588 (800)\ttotal: 2m 25s\tremaining: 35.3s\n",
      "805:\ttest: 0.8749490\tbest: 0.8749588 (800)\ttotal: 2m 25s\tremaining: 35.1s\n",
      "806:\ttest: 0.8749555\tbest: 0.8749588 (800)\ttotal: 2m 25s\tremaining: 34.9s\n",
      "807:\ttest: 0.8749437\tbest: 0.8749588 (800)\ttotal: 2m 26s\tremaining: 34.7s\n",
      "808:\ttest: 0.8749465\tbest: 0.8749588 (800)\ttotal: 2m 26s\tremaining: 34.5s\n",
      "809:\ttest: 0.8749431\tbest: 0.8749588 (800)\ttotal: 2m 26s\tremaining: 34.3s\n",
      "810:\ttest: 0.8749603\tbest: 0.8749603 (810)\ttotal: 2m 26s\tremaining: 34.2s\n",
      "811:\ttest: 0.8749835\tbest: 0.8749835 (811)\ttotal: 2m 26s\tremaining: 34s\n",
      "812:\ttest: 0.8749917\tbest: 0.8749917 (812)\ttotal: 2m 26s\tremaining: 33.8s\n",
      "813:\ttest: 0.8750008\tbest: 0.8750008 (813)\ttotal: 2m 27s\tremaining: 33.6s\n",
      "814:\ttest: 0.8750289\tbest: 0.8750289 (814)\ttotal: 2m 27s\tremaining: 33.4s\n",
      "815:\ttest: 0.8750242\tbest: 0.8750289 (814)\ttotal: 2m 27s\tremaining: 33.2s\n",
      "816:\ttest: 0.8750298\tbest: 0.8750298 (816)\ttotal: 2m 27s\tremaining: 33.1s\n",
      "817:\ttest: 0.8750294\tbest: 0.8750298 (816)\ttotal: 2m 27s\tremaining: 32.9s\n",
      "818:\ttest: 0.8750321\tbest: 0.8750321 (818)\ttotal: 2m 27s\tremaining: 32.7s\n",
      "819:\ttest: 0.8750376\tbest: 0.8750376 (819)\ttotal: 2m 28s\tremaining: 32.5s\n",
      "820:\ttest: 0.8750441\tbest: 0.8750441 (820)\ttotal: 2m 28s\tremaining: 32.3s\n",
      "821:\ttest: 0.8750477\tbest: 0.8750477 (821)\ttotal: 2m 28s\tremaining: 32.1s\n",
      "822:\ttest: 0.8750442\tbest: 0.8750477 (821)\ttotal: 2m 28s\tremaining: 32s\n",
      "823:\ttest: 0.8750547\tbest: 0.8750547 (823)\ttotal: 2m 28s\tremaining: 31.8s\n",
      "824:\ttest: 0.8750622\tbest: 0.8750622 (824)\ttotal: 2m 28s\tremaining: 31.6s\n",
      "825:\ttest: 0.8750746\tbest: 0.8750746 (825)\ttotal: 2m 29s\tremaining: 31.4s\n",
      "826:\ttest: 0.8750955\tbest: 0.8750955 (826)\ttotal: 2m 29s\tremaining: 31.2s\n",
      "827:\ttest: 0.8750865\tbest: 0.8750955 (826)\ttotal: 2m 29s\tremaining: 31.1s\n",
      "828:\ttest: 0.8750808\tbest: 0.8750955 (826)\ttotal: 2m 29s\tremaining: 30.9s\n",
      "829:\ttest: 0.8750903\tbest: 0.8750955 (826)\ttotal: 2m 29s\tremaining: 30.7s\n",
      "830:\ttest: 0.8750957\tbest: 0.8750957 (830)\ttotal: 2m 29s\tremaining: 30.5s\n",
      "831:\ttest: 0.8751006\tbest: 0.8751006 (831)\ttotal: 2m 30s\tremaining: 30.3s\n",
      "832:\ttest: 0.8751249\tbest: 0.8751249 (832)\ttotal: 2m 30s\tremaining: 30.1s\n",
      "833:\ttest: 0.8751244\tbest: 0.8751249 (832)\ttotal: 2m 30s\tremaining: 30s\n",
      "834:\ttest: 0.8751260\tbest: 0.8751260 (834)\ttotal: 2m 30s\tremaining: 29.8s\n",
      "835:\ttest: 0.8751363\tbest: 0.8751363 (835)\ttotal: 2m 30s\tremaining: 29.6s\n",
      "836:\ttest: 0.8751200\tbest: 0.8751363 (835)\ttotal: 2m 31s\tremaining: 29.4s\n",
      "837:\ttest: 0.8751162\tbest: 0.8751363 (835)\ttotal: 2m 31s\tremaining: 29.2s\n",
      "838:\ttest: 0.8751161\tbest: 0.8751363 (835)\ttotal: 2m 31s\tremaining: 29s\n",
      "839:\ttest: 0.8751168\tbest: 0.8751363 (835)\ttotal: 2m 31s\tremaining: 28.9s\n",
      "840:\ttest: 0.8751370\tbest: 0.8751370 (840)\ttotal: 2m 31s\tremaining: 28.7s\n",
      "841:\ttest: 0.8751488\tbest: 0.8751488 (841)\ttotal: 2m 31s\tremaining: 28.5s\n",
      "842:\ttest: 0.8751451\tbest: 0.8751488 (841)\ttotal: 2m 32s\tremaining: 28.3s\n",
      "843:\ttest: 0.8751643\tbest: 0.8751643 (843)\ttotal: 2m 32s\tremaining: 28.1s\n",
      "844:\ttest: 0.8751735\tbest: 0.8751735 (844)\ttotal: 2m 32s\tremaining: 28s\n",
      "845:\ttest: 0.8752066\tbest: 0.8752066 (845)\ttotal: 2m 32s\tremaining: 27.8s\n",
      "846:\ttest: 0.8752237\tbest: 0.8752237 (846)\ttotal: 2m 32s\tremaining: 27.6s\n",
      "847:\ttest: 0.8752879\tbest: 0.8752879 (847)\ttotal: 2m 33s\tremaining: 27.4s\n",
      "848:\ttest: 0.8752832\tbest: 0.8752879 (847)\ttotal: 2m 33s\tremaining: 27.2s\n",
      "849:\ttest: 0.8753146\tbest: 0.8753146 (849)\ttotal: 2m 33s\tremaining: 27.1s\n",
      "850:\ttest: 0.8753095\tbest: 0.8753146 (849)\ttotal: 2m 33s\tremaining: 26.9s\n",
      "851:\ttest: 0.8753084\tbest: 0.8753146 (849)\ttotal: 2m 33s\tremaining: 26.7s\n",
      "852:\ttest: 0.8753094\tbest: 0.8753146 (849)\ttotal: 2m 33s\tremaining: 26.5s\n",
      "853:\ttest: 0.8753389\tbest: 0.8753389 (853)\ttotal: 2m 33s\tremaining: 26.3s\n",
      "854:\ttest: 0.8753409\tbest: 0.8753409 (854)\ttotal: 2m 34s\tremaining: 26.1s\n",
      "855:\ttest: 0.8753294\tbest: 0.8753409 (854)\ttotal: 2m 34s\tremaining: 26s\n",
      "856:\ttest: 0.8753323\tbest: 0.8753409 (854)\ttotal: 2m 34s\tremaining: 25.8s\n",
      "857:\ttest: 0.8753348\tbest: 0.8753409 (854)\ttotal: 2m 34s\tremaining: 25.6s\n",
      "858:\ttest: 0.8753446\tbest: 0.8753446 (858)\ttotal: 2m 34s\tremaining: 25.4s\n",
      "859:\ttest: 0.8753510\tbest: 0.8753510 (859)\ttotal: 2m 35s\tremaining: 25.2s\n",
      "860:\ttest: 0.8753448\tbest: 0.8753510 (859)\ttotal: 2m 35s\tremaining: 25.1s\n",
      "861:\ttest: 0.8753430\tbest: 0.8753510 (859)\ttotal: 2m 35s\tremaining: 24.9s\n",
      "862:\ttest: 0.8754117\tbest: 0.8754117 (862)\ttotal: 2m 35s\tremaining: 24.7s\n",
      "863:\ttest: 0.8754082\tbest: 0.8754117 (862)\ttotal: 2m 35s\tremaining: 24.5s\n",
      "864:\ttest: 0.8754320\tbest: 0.8754320 (864)\ttotal: 2m 35s\tremaining: 24.3s\n",
      "865:\ttest: 0.8754141\tbest: 0.8754320 (864)\ttotal: 2m 36s\tremaining: 24.1s\n",
      "866:\ttest: 0.8754277\tbest: 0.8754320 (864)\ttotal: 2m 36s\tremaining: 24s\n",
      "867:\ttest: 0.8754343\tbest: 0.8754343 (867)\ttotal: 2m 36s\tremaining: 23.8s\n",
      "868:\ttest: 0.8754356\tbest: 0.8754356 (868)\ttotal: 2m 36s\tremaining: 23.6s\n",
      "869:\ttest: 0.8754340\tbest: 0.8754356 (868)\ttotal: 2m 36s\tremaining: 23.4s\n",
      "870:\ttest: 0.8754313\tbest: 0.8754356 (868)\ttotal: 2m 36s\tremaining: 23.2s\n",
      "871:\ttest: 0.8754735\tbest: 0.8754735 (871)\ttotal: 2m 37s\tremaining: 23.1s\n",
      "872:\ttest: 0.8755023\tbest: 0.8755023 (872)\ttotal: 2m 37s\tremaining: 22.9s\n",
      "873:\ttest: 0.8755018\tbest: 0.8755023 (872)\ttotal: 2m 37s\tremaining: 22.7s\n",
      "874:\ttest: 0.8755167\tbest: 0.8755167 (874)\ttotal: 2m 37s\tremaining: 22.5s\n",
      "875:\ttest: 0.8755258\tbest: 0.8755258 (875)\ttotal: 2m 37s\tremaining: 22.3s\n",
      "876:\ttest: 0.8755265\tbest: 0.8755265 (876)\ttotal: 2m 37s\tremaining: 22.1s\n",
      "877:\ttest: 0.8755379\tbest: 0.8755379 (877)\ttotal: 2m 38s\tremaining: 22s\n",
      "878:\ttest: 0.8755511\tbest: 0.8755511 (878)\ttotal: 2m 38s\tremaining: 21.8s\n",
      "879:\ttest: 0.8755443\tbest: 0.8755511 (878)\ttotal: 2m 38s\tremaining: 21.6s\n",
      "880:\ttest: 0.8755498\tbest: 0.8755511 (878)\ttotal: 2m 38s\tremaining: 21.4s\n",
      "881:\ttest: 0.8755605\tbest: 0.8755605 (881)\ttotal: 2m 38s\tremaining: 21.2s\n",
      "882:\ttest: 0.8755757\tbest: 0.8755757 (882)\ttotal: 2m 38s\tremaining: 21.1s\n",
      "883:\ttest: 0.8755736\tbest: 0.8755757 (882)\ttotal: 2m 39s\tremaining: 20.9s\n",
      "884:\ttest: 0.8755841\tbest: 0.8755841 (884)\ttotal: 2m 39s\tremaining: 20.7s\n",
      "885:\ttest: 0.8755865\tbest: 0.8755865 (885)\ttotal: 2m 39s\tremaining: 20.5s\n",
      "886:\ttest: 0.8756126\tbest: 0.8756126 (886)\ttotal: 2m 39s\tremaining: 20.3s\n",
      "887:\ttest: 0.8756054\tbest: 0.8756126 (886)\ttotal: 2m 39s\tremaining: 20.2s\n",
      "888:\ttest: 0.8756080\tbest: 0.8756126 (886)\ttotal: 2m 40s\tremaining: 20s\n",
      "889:\ttest: 0.8756083\tbest: 0.8756126 (886)\ttotal: 2m 40s\tremaining: 19.8s\n",
      "890:\ttest: 0.8756057\tbest: 0.8756126 (886)\ttotal: 2m 40s\tremaining: 19.6s\n",
      "891:\ttest: 0.8756134\tbest: 0.8756134 (891)\ttotal: 2m 40s\tremaining: 19.4s\n",
      "892:\ttest: 0.8756224\tbest: 0.8756224 (892)\ttotal: 2m 40s\tremaining: 19.3s\n",
      "893:\ttest: 0.8756378\tbest: 0.8756378 (893)\ttotal: 2m 40s\tremaining: 19.1s\n",
      "894:\ttest: 0.8756374\tbest: 0.8756378 (893)\ttotal: 2m 41s\tremaining: 18.9s\n",
      "895:\ttest: 0.8756317\tbest: 0.8756378 (893)\ttotal: 2m 41s\tremaining: 18.7s\n",
      "896:\ttest: 0.8756257\tbest: 0.8756378 (893)\ttotal: 2m 41s\tremaining: 18.5s\n",
      "897:\ttest: 0.8756709\tbest: 0.8756709 (897)\ttotal: 2m 41s\tremaining: 18.3s\n",
      "898:\ttest: 0.8756702\tbest: 0.8756709 (897)\ttotal: 2m 41s\tremaining: 18.2s\n",
      "899:\ttest: 0.8756678\tbest: 0.8756709 (897)\ttotal: 2m 41s\tremaining: 18s\n",
      "900:\ttest: 0.8756688\tbest: 0.8756709 (897)\ttotal: 2m 42s\tremaining: 17.8s\n",
      "901:\ttest: 0.8756726\tbest: 0.8756726 (901)\ttotal: 2m 42s\tremaining: 17.6s\n",
      "902:\ttest: 0.8756785\tbest: 0.8756785 (902)\ttotal: 2m 42s\tremaining: 17.4s\n",
      "903:\ttest: 0.8757015\tbest: 0.8757015 (903)\ttotal: 2m 42s\tremaining: 17.3s\n",
      "904:\ttest: 0.8757002\tbest: 0.8757015 (903)\ttotal: 2m 42s\tremaining: 17.1s\n",
      "905:\ttest: 0.8757007\tbest: 0.8757015 (903)\ttotal: 2m 42s\tremaining: 16.9s\n",
      "906:\ttest: 0.8756995\tbest: 0.8757015 (903)\ttotal: 2m 43s\tremaining: 16.7s\n",
      "907:\ttest: 0.8757018\tbest: 0.8757018 (907)\ttotal: 2m 43s\tremaining: 16.5s\n",
      "908:\ttest: 0.8757306\tbest: 0.8757306 (908)\ttotal: 2m 43s\tremaining: 16.4s\n",
      "909:\ttest: 0.8757534\tbest: 0.8757534 (909)\ttotal: 2m 43s\tremaining: 16.2s\n",
      "910:\ttest: 0.8757486\tbest: 0.8757534 (909)\ttotal: 2m 43s\tremaining: 16s\n",
      "911:\ttest: 0.8757416\tbest: 0.8757534 (909)\ttotal: 2m 43s\tremaining: 15.8s\n",
      "912:\ttest: 0.8757406\tbest: 0.8757534 (909)\ttotal: 2m 44s\tremaining: 15.6s\n",
      "913:\ttest: 0.8757489\tbest: 0.8757534 (909)\ttotal: 2m 44s\tremaining: 15.5s\n",
      "914:\ttest: 0.8757509\tbest: 0.8757534 (909)\ttotal: 2m 44s\tremaining: 15.3s\n",
      "915:\ttest: 0.8757511\tbest: 0.8757534 (909)\ttotal: 2m 44s\tremaining: 15.1s\n",
      "916:\ttest: 0.8757541\tbest: 0.8757541 (916)\ttotal: 2m 44s\tremaining: 14.9s\n",
      "917:\ttest: 0.8757489\tbest: 0.8757541 (916)\ttotal: 2m 44s\tremaining: 14.7s\n",
      "918:\ttest: 0.8757598\tbest: 0.8757598 (918)\ttotal: 2m 45s\tremaining: 14.6s\n",
      "919:\ttest: 0.8757742\tbest: 0.8757742 (919)\ttotal: 2m 45s\tremaining: 14.4s\n",
      "920:\ttest: 0.8757838\tbest: 0.8757838 (920)\ttotal: 2m 45s\tremaining: 14.2s\n",
      "921:\ttest: 0.8757861\tbest: 0.8757861 (921)\ttotal: 2m 45s\tremaining: 14s\n",
      "922:\ttest: 0.8757737\tbest: 0.8757861 (921)\ttotal: 2m 45s\tremaining: 13.8s\n",
      "923:\ttest: 0.8757873\tbest: 0.8757873 (923)\ttotal: 2m 46s\tremaining: 13.7s\n",
      "924:\ttest: 0.8758003\tbest: 0.8758003 (924)\ttotal: 2m 46s\tremaining: 13.5s\n",
      "925:\ttest: 0.8758055\tbest: 0.8758055 (925)\ttotal: 2m 46s\tremaining: 13.3s\n",
      "926:\ttest: 0.8758279\tbest: 0.8758279 (926)\ttotal: 2m 46s\tremaining: 13.1s\n",
      "927:\ttest: 0.8758754\tbest: 0.8758754 (927)\ttotal: 2m 46s\tremaining: 12.9s\n",
      "928:\ttest: 0.8758728\tbest: 0.8758754 (927)\ttotal: 2m 46s\tremaining: 12.8s\n",
      "929:\ttest: 0.8758878\tbest: 0.8758878 (929)\ttotal: 2m 47s\tremaining: 12.6s\n",
      "930:\ttest: 0.8758772\tbest: 0.8758878 (929)\ttotal: 2m 47s\tremaining: 12.4s\n",
      "931:\ttest: 0.8758865\tbest: 0.8758878 (929)\ttotal: 2m 47s\tremaining: 12.2s\n",
      "932:\ttest: 0.8759797\tbest: 0.8759797 (932)\ttotal: 2m 47s\tremaining: 12s\n",
      "933:\ttest: 0.8760068\tbest: 0.8760068 (933)\ttotal: 2m 47s\tremaining: 11.9s\n",
      "934:\ttest: 0.8760069\tbest: 0.8760069 (934)\ttotal: 2m 47s\tremaining: 11.7s\n",
      "935:\ttest: 0.8760175\tbest: 0.8760175 (935)\ttotal: 2m 48s\tremaining: 11.5s\n",
      "936:\ttest: 0.8760075\tbest: 0.8760175 (935)\ttotal: 2m 48s\tremaining: 11.3s\n",
      "937:\ttest: 0.8760115\tbest: 0.8760175 (935)\ttotal: 2m 48s\tremaining: 11.1s\n",
      "938:\ttest: 0.8760047\tbest: 0.8760175 (935)\ttotal: 2m 48s\tremaining: 11s\n",
      "939:\ttest: 0.8760200\tbest: 0.8760200 (939)\ttotal: 2m 48s\tremaining: 10.8s\n",
      "940:\ttest: 0.8760588\tbest: 0.8760588 (940)\ttotal: 2m 48s\tremaining: 10.6s\n",
      "941:\ttest: 0.8760608\tbest: 0.8760608 (941)\ttotal: 2m 49s\tremaining: 10.4s\n",
      "942:\ttest: 0.8760718\tbest: 0.8760718 (942)\ttotal: 2m 49s\tremaining: 10.2s\n",
      "943:\ttest: 0.8760675\tbest: 0.8760718 (942)\ttotal: 2m 49s\tremaining: 10.1s\n",
      "944:\ttest: 0.8760555\tbest: 0.8760718 (942)\ttotal: 2m 49s\tremaining: 9.87s\n",
      "945:\ttest: 0.8760599\tbest: 0.8760718 (942)\ttotal: 2m 49s\tremaining: 9.69s\n",
      "946:\ttest: 0.8760721\tbest: 0.8760721 (946)\ttotal: 2m 49s\tremaining: 9.51s\n",
      "947:\ttest: 0.8760641\tbest: 0.8760721 (946)\ttotal: 2m 50s\tremaining: 9.33s\n",
      "948:\ttest: 0.8760524\tbest: 0.8760721 (946)\ttotal: 2m 50s\tremaining: 9.15s\n",
      "949:\ttest: 0.8760508\tbest: 0.8760721 (946)\ttotal: 2m 50s\tremaining: 8.97s\n",
      "950:\ttest: 0.8760584\tbest: 0.8760721 (946)\ttotal: 2m 50s\tremaining: 8.79s\n",
      "951:\ttest: 0.8760614\tbest: 0.8760721 (946)\ttotal: 2m 50s\tremaining: 8.61s\n",
      "952:\ttest: 0.8760713\tbest: 0.8760721 (946)\ttotal: 2m 50s\tremaining: 8.43s\n",
      "953:\ttest: 0.8760706\tbest: 0.8760721 (946)\ttotal: 2m 51s\tremaining: 8.25s\n",
      "954:\ttest: 0.8760343\tbest: 0.8760721 (946)\ttotal: 2m 51s\tremaining: 8.07s\n",
      "955:\ttest: 0.8760420\tbest: 0.8760721 (946)\ttotal: 2m 51s\tremaining: 7.89s\n",
      "956:\ttest: 0.8760467\tbest: 0.8760721 (946)\ttotal: 2m 51s\tremaining: 7.71s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.8760720894\n",
      "bestIteration = 946\n",
      "\n",
      "Shrink model to first 947 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x74b8e877c310>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool=catboost.Pool(df, target)\n",
    "valid_pool=catboost.Pool(X_dev, y_dev)\n",
    "cat_model=CatBoostClassifier(eval_metric='AUC', \n",
    "                             learning_rate=0.05,\n",
    "                             max_depth=6,\n",
    "                         random_seed=0,\n",
    "                         task_type='CPU'\n",
    "                        )\n",
    "cat_model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7303913883610675)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_metric([cat_model], X_dev, y_dev, dev_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 0.7303913883610675\n",
      "0 0.1 0.9 0.7327129523206786\n",
      "0 0.2 0.8 0.7344863422700869\n",
      "0 0.3 0.7 0.7360531322309033\n",
      "0 0.4 0.6 0.7372951191424512\n",
      "0 0.5 0.5 0.7380950110556241\n",
      "0 0.6 0.4 0.7387161372094925\n",
      "0 0.7 0.30000000000000004 0.7389370787076727\n",
      "0 0.8 0.19999999999999996 0.738982646668712\n"
     ]
    }
   ],
   "source": [
    "best_score=0\n",
    "for w0, w1 in itertools.product([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], repeat=2):\n",
    "    weeks=[]\n",
    "    ginis=[]\n",
    "    for week in sorted(dev_weeks.unique()):\n",
    "        mask=dev_weeks==week\n",
    "        y_pred=w0*predict([lgb_model], X_dev[mask])+w1*predict([xgb_model], X_dev[mask])+(1-w0-w1)*predict([cat_model], X_dev[mask])\n",
    "        y_true=y_dev[mask]\n",
    "        gini=2*roc_auc_score(y_true, y_pred)-1\n",
    "        weeks.append(week)\n",
    "        ginis.append(gini)\n",
    "    slope,intercept,_,_,_=linregress(weeks, ginis)            \n",
    "    std=np.std([slope*week+intercept-gini for week, gini in zip(weeks, ginis)])\n",
    "    final_score= np.mean(ginis)+88*min(0,slope)-0.5*std\n",
    "    if final_score>best_score:\n",
    "        best_score=final_score\n",
    "        print(w0, w1, 1-w0-w1, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks=[]\n",
    "ginis=[]\n",
    "for week in sorted(dev_weeks.unique()):\n",
    "    mask=dev_weeks==week\n",
    "    y_pred=0*predict([lgb_model], X_dev[mask])+0.8*predict([xgb_model], X_dev[mask])+0.2*predict([cat_model], X_dev[mask])\n",
    "    y_true=y_dev[mask]\n",
    "    gini=2*roc_auc_score(y_true, y_pred)-1\n",
    "    weeks.append(week)\n",
    "    ginis.append(gini)\n",
    "slope,intercept,_,_,_=linregress(weeks, ginis)            \n",
    "std=np.std([slope*week+intercept-gini for week, gini in zip(weeks, ginis)])\n",
    "final_score= np.mean(ginis)+88*min(0,slope)-0.5*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=0*predict([lgb_model], X_dev)+0.8*predict([xgb_model], X_dev)+0.2*predict([cat_model], X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_dict={col:value for col,value in zip(X_dev.columns,lgb_model.feature_importance())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_dict=dict(sorted(fi_dict.items(), key= lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_cols=[col for col, value in fi_dict.items() if value!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=abs(y_dev-y_pred)>0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7310481212920237"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask)/len(X_dev)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=X_dev[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=X_dev[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1214/3166032428.py:3: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  stat, pval = ks_2samp(df1[col][~df1[col].isnull()],df2[col][~df2[col].isnull()])\n"
     ]
    }
   ],
   "source": [
    "ks_dict={}\n",
    "for col in [col for col in num_cols if col not in []]:\n",
    "    stat, pval = ks_2samp(df1[col][~df1[col].isnull()],df2[col][~df2[col].isnull()])\n",
    "    ks_dict[col]= stat, pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_dict=dict(sorted(ks_dict.items(), key= lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_cols=[col for col, value in ks_dict.items() if value[1] <0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ks_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([col for col in ks_cols if col in fi_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[col for col in ks_cols if col in fi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "cols=[]\n",
    "count=0\n",
    "for col in [col for col in ks_cols if col in fi_cols]:\n",
    "    if fi_cols.index(col)<10:\n",
    "        count+=1\n",
    "        cols.append(col)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_dict={}\n",
    "for col in cat_cols:\n",
    "    contingency_table=pd.concat([df1[col].value_counts(),df2[col].value_counts()], axis=1).fillna(0)\n",
    "    stat, pval, _, _ = chi2_contingency(contingency_table)\n",
    "    chi2_dict[col]=stat, pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_dict=dict(sorted(chi2_dict.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_cols=[col for col, value in chi2_dict.items() if value[1] <0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chi2_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([col for col in chi2_cols if col in fi_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[col for col in chi2_cols if col in fi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for col in [col for col in chi2_cols if col in fi_cols]:\n",
    "    if fi_cols.index(col)<10:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7229123224740361)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_metric([lgb_model], X_test, y_test, test_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7227238532362731)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_metric([xgb_model], X_test, y_test, test_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7137661608965555)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_metric([cat_model], X_test, y_test, test_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks=[]\n",
    "ginis=[]\n",
    "for week in sorted(test_weeks.unique()):\n",
    "    mask=test_weeks==week\n",
    "    y_pred=0*predict([lgb_model], X_test[mask])+0.8*predict([xgb_model], X_test[mask])+0.2*predict([cat_model], X_test[mask])\n",
    "    y_true=y_test[mask]\n",
    "    gini=2*roc_auc_score(y_true, y_pred)-1\n",
    "    weeks.append(week)\n",
    "    ginis.append(gini)\n",
    "slope,intercept,_,_,_=linregress(weeks, ginis)            \n",
    "std=np.std([slope*week+intercept-gini for week, gini in zip(weeks, ginis)])\n",
    "final_score= np.mean(ginis)+88*min(0,slope)-0.5*std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7238556591144566)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "isSourceIdPinned": false,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "datasetId": 7148037,
     "sourceId": 11412708,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 299312,
     "modelInstanceId": 278406,
     "sourceId": 332120,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
